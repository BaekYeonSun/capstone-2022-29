{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_r3RBbn79gc"
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !apt-get update\n",
        "# !apt-get install g++ openjdk-8-jdk \n",
        "# !pip3 install konlpy JPype1-py3\n",
        "# !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNnu6pX7x8HK"
      },
      "outputs": [],
      "source": [
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "mecab = Mecab()\n",
        "mecab.morphs('동해물과 백두산이 마르고 닳도록')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboardx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4RRNgWeGGAd"
      },
      "source": [
        "## pyrouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pyrouge --upgrade\n",
        "!pip install https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
        "!pip install pyrouge\n",
        "!pip show pyrouge\n",
        "!git clone https://github.com/andersjo/pyrouge.git\n",
        "from pyrouge import Rouge155\n",
        "!pyrouge_set_rouge_path 'pyrouge/tools/ROUGE-1.5.5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!sudo apt-get update -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!sudo apt-get install -y libxml-parser-perl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%shell\n",
        "!cd pyrouge/tools/ROUGE-1.5.5/data\n",
        "!rm WordNet-2.0.exc.db # only if exist\n",
        "!cd WordNet-2.0-Exceptions\n",
        "!rm WordNet-2.0.exc.db # only if exist\n",
        "\n",
        "!./buildExeptionDB.pl . exc WordNet-2.0.exc.db\n",
        "!cd ../\n",
        "!ln -s WordNet-2.0-Exceptions/WordNet-2.0.exc.db WordNet-2.0.exc.db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i6OGXdMn8Ba"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "config = BertConfig.from_pretrained('bert-base-multilingual-cased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjHhHI9zm8Y9"
      },
      "source": [
        "## Load csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6IWz6c4Ifjar"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "train = pd.read_csv('json_to_csv_train_신문기사.csv').drop('Unnamed: 0', axis=1)\n",
        "print(len(train))\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "valid = pd.read_csv('json_to_csv_valid_신문기사.csv').drop('Unnamed: 0', axis=1)\n",
        "print(len(valid))\n",
        "valid.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iKRc5sBr7cK8"
      },
      "outputs": [],
      "source": [
        "all_data = pd.concat([train,valid])\n",
        "print(len(all_data))\n",
        "all_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set = train\n",
        "valid_set, test_set = train_test_split(valid, test_size = 0.1)\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train_set, test_set = train_test_split(all_data, test_size = 0.2)\n",
        "# valid_set, test_set = train_test_split(test_set, test_size = 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(train_set))\n",
        "print(len(valid_set))\n",
        "print(len(test_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNxZvpLWg195"
      },
      "source": [
        "## csv to json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "list_dic = []\n",
        "for idx, row in train_set.iterrows():\n",
        "  raw = row['article_orginal']\n",
        "  target_idx = ast.literal_eval(row['extractive'])\n",
        "\n",
        "  sentences = raw.split(',')\n",
        "  src = [i.split(' ') for i in sentences]\n",
        "  tgt = [a for i,a in enumerate(src) if i in target_idx]\n",
        "  \n",
        "  mydict = {}\n",
        "  mydict['src'] = src\n",
        "  mydict['tgt'] = tgt\n",
        "  list_dic.append(mydict)\n",
        "        \n",
        "temp = []\n",
        "for i,a in enumerate(tqdm(list_dic)):\n",
        "  if (i+1)%1000!=0:\n",
        "      temp.append(a)\n",
        "  else:\n",
        "      filename = 'korean.'+'train'+'.'+str(i//1000)+'.json'\n",
        "      with open('KorBertSum/json/'+filename, \"w\", encoding='utf-8') as json_file:\n",
        "          json.dump(temp, json_file, ensure_ascii=False)\n",
        "      temp = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Csh4zkpPJpzu"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  os.mkdir('KorBertSum/json/val')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "list_dic = []\n",
        "for idx, row in valid_set.iterrows():\n",
        "        raw = row['article_orginal']\n",
        "        target_idx = ast.literal_eval(row['extractive'])\n",
        "\n",
        "        sentences = raw.split(',')\n",
        "        src = [i.split(' ') for i in sentences]\n",
        "        tgt = [a for i,a in enumerate(src) if i in target_idx]\n",
        "        \n",
        "        mydict = {}\n",
        "        mydict['src'] = src\n",
        "        mydict['tgt'] = tgt\n",
        "        list_dic.append(mydict)\n",
        "        \n",
        "temp = []\n",
        "for i,a in enumerate(tqdm(list_dic)):\n",
        "    \n",
        "    if (i+1)%1000!=0:\n",
        "        temp.append(a)\n",
        "    else:\n",
        "        filename = 'korean.'+'valid'+'.'+str(i//1000)+'.json'\n",
        "        with open('KorBertSum/json/val/'+filename, \"w\", encoding='utf-8') as json_file:\n",
        "            json.dump(temp, json_file, ensure_ascii=False)\n",
        "        temp = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WTE25bv8Jp3A"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  os.mkdir('KorBertSum/json/test')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "list_dic = []\n",
        "for idx, row in valid_set.iterrows():\n",
        "        raw = row['article_orginal']\n",
        "        target_idx = ast.literal_eval(row['extractive'])\n",
        "\n",
        "        sentences = raw.split(',')\n",
        "        src = [i.split(' ') for i in sentences]\n",
        "        tgt = [a for i,a in enumerate(src) if i in target_idx]\n",
        "        \n",
        "        mydict = {}\n",
        "        mydict['src'] = src\n",
        "        mydict['tgt'] = tgt\n",
        "        list_dic.append(mydict)\n",
        "        \n",
        "temp = []\n",
        "for i,a in enumerate(tqdm(list_dic)):\n",
        "    \n",
        "    if (i+1)%1000!=0:\n",
        "        temp.append(a)\n",
        "    else:\n",
        "        filename = 'korean.'+'test'+'.'+str(i//1000)+'.json'\n",
        "        with open('KorBertSum/json/test/'+filename, \"w\", encoding='utf-8') as json_file:\n",
        "            json.dump(temp, json_file, ensure_ascii=False)\n",
        "        temp = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD6y73O4fmkY"
      },
      "source": [
        "## Json to pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tLQfbpzEKS6b"
      },
      "outputs": [],
      "source": [
        "# 데이터 처리모드, json경로, 저장경로, 로그파일\n",
        "!python KorBertSum/src/preprocess.py \\\n",
        "  -mode format_to_bert \\\n",
        "  -raw_path KorBertSum/json \\\n",
        "  -save_path KorBertSum/bert_data \\\n",
        "  -dataset train \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tGiQq9ejKI47"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  os.mkdir('KorBertSum/bert_data/test')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "!python KorBertSum/src/preprocess.py \\\n",
        "  -mode format_to_bert \\\n",
        "  -raw_path KorBertSum/json/val \\\n",
        "  -save_path KorBertSum/bert_data/test \\\n",
        "  -dataset valid \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xDhVi3KQJ3_c"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  os.mkdir('KorBertSum/bert_data/test')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "!python KorBertSum/src/preprocess.py \\\n",
        "  -mode format_to_bert \\\n",
        "  -raw_path KorBertSum/json/test \\\n",
        "  -save_path KorBertSum/bert_data/test \\\n",
        "  -dataset test \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa5vcawrGlbo"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nHnvBZO-cwbf"
      },
      "outputs": [],
      "source": [
        "!python KorBertSum/src/train.py \\\n",
        "  -mode train \\\n",
        "  -encoder classifier \\\n",
        "  -bert_data_path KorBertSum/bert_data/korean \\\n",
        "  -model_path KorBertSum/models/bert_classifier \\\n",
        "  -log_file KorBertSum/logs/train.txt \\\n",
        "  -lr 0.001 -visible_gpus 0 -gpu_ranks 0 -world_size 1 -report_every 1000 -dropout 0.5 \\\n",
        "  -save_checkpoint_steps 5000 -batch_size 3000 -decay_method noam -train_steps 35000 -accum_count 1 \\\n",
        "#   -optim adamW \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWd-qpB_GoeE"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7XV4tywmcT_P"
      },
      "outputs": [],
      "source": [
        "!python KorBertSum/src/train.py \\\n",
        "  -mode validate \\\n",
        "  -bert_data_path KorBertSum/bert_data/test/korean \\\n",
        "  -model_path KorBertSum/models/bert_classifier \\\n",
        "  -visible_gpus 0 -gpu_ranks 0 -batch_size 3000 \\\n",
        "  -log_file KorBertSum/logs/val.txt \\\n",
        "  -temp_dir KorBertSum/temp \\\n",
        "  -result_path KorBertSum/results \\\n",
        "#   -test_all 1\n",
        "#   -test_from KorBertSum/models/bert_classifier/model_step_30000.pt \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw_x2sIFnGOP"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nisU6_FZEGfx"
      },
      "outputs": [],
      "source": [
        "!python KorBertSum/src/train.py \\\n",
        "  -mode test \\\n",
        "  -bert_data_path KorBertSum/bert_data/test/korean \\\n",
        "  -visible_gpus 0 -gpu_ranks 0 -batch_size 3000 \\\n",
        "  -log_file KorBertSum/logs/test.txt \\\n",
        "  -temp_dir KorBertSum/temp \\\n",
        "  -result_path KorBertSum/results \\\n",
        "  -test_from KorBertSum/models/bert_classifier/model_step_35000.pt \\"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "BertSum",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
