{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch==1.8.1+cpu torchvision==0.9.1+cpu torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pyrouge --upgrade\n",
        "!pip install https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
        "!pip install pyrouge\n",
        "!pip show pyrouge\n",
        "!git clone https://github.com/andersjo/pyrouge.git\n",
        "from pyrouge import Rouge155\n",
        "!pyrouge_set_rouge_path 'pyrouge/tools/ROUGE-1.5.5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboardX\n",
        "!pip install easydict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir('KorBertSum/src')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from models import data_loader, model_builder\n",
        "from models.model_builder import Summarizer\n",
        "from others.logging import logger, init_logger\n",
        "from models.data_loader import load_dataset\n",
        "from transformers import BertConfig, BertTokenizer\n",
        "from tensorboardX import SummaryWriter\n",
        "from models.reporter import ReportMgr\n",
        "from models.stats import Statistics\n",
        "import easydict\n",
        "from multiprocessing.dummy import Pool as ThreadPool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "NUM_CORES = multiprocessing.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(NUM_CORES) # 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _tally_parameters(model):\n",
        "    n_params = sum([p.nelement() for p in model.parameters()])\n",
        "    return n_params\n",
        "\n",
        "def build_trainer(args, device_id, model,\n",
        "                  optim):\n",
        "    \"\"\"\n",
        "    Simplify `Trainer` creation based on user `opt`s*\n",
        "    Args:\n",
        "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
        "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
        "        fields (dict): dict of fields\n",
        "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
        "        data_type (str): string describing the type of data\n",
        "            e.g. \"text\", \"img\", \"audio\"\n",
        "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
        "            used to save the model\n",
        "    \"\"\"\n",
        "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "\n",
        "\n",
        "    grad_accum_count = args.accum_count\n",
        "    n_gpu = args.world_size\n",
        "\n",
        "    if device_id >= 0:\n",
        "        gpu_rank = int(args.gpu_ranks[device_id])\n",
        "    else:\n",
        "        gpu_rank = 0\n",
        "        n_gpu = 0\n",
        "\n",
        "    print('gpu_rank %d' % gpu_rank)\n",
        "\n",
        "    tensorboard_log_dir = args.model_path\n",
        "\n",
        "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
        "\n",
        "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
        "\n",
        "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
        "\n",
        "    # print(tr)\n",
        "    if (model):\n",
        "        n_params = _tally_parameters(model)\n",
        "        logger.info('* number of parameters: %d' % n_params)\n",
        "\n",
        "    return trainer\n",
        "\n",
        "class Trainer(object):\n",
        "    \"\"\"\n",
        "    Class that controls the training process.\n",
        "\n",
        "    Args:\n",
        "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
        "                to train\n",
        "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
        "               training loss computation\n",
        "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
        "               training loss computation\n",
        "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
        "               the optimizer responsible for update\n",
        "            trunc_size(int): length of truncated back propagation through time\n",
        "            shard_size(int): compute loss in shards of this size for efficiency\n",
        "            data_type(string): type of the source input: [text|img|audio]\n",
        "            norm_method(string): normalization methods: [sents|tokens]\n",
        "            grad_accum_count(int): accumulate gradients this many times.\n",
        "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
        "                the object that creates reports, or None\n",
        "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
        "                used to save a checkpoint.\n",
        "                Thus nothing will be saved if this parameter is None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,  args, model,  optim,\n",
        "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
        "                  report_manager=None):\n",
        "        # Basic attributes.\n",
        "        self.args = args\n",
        "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
        "        self.model = model\n",
        "        self.optim = optim\n",
        "        self.grad_accum_count = grad_accum_count\n",
        "        self.n_gpu = n_gpu\n",
        "        self.gpu_rank = gpu_rank\n",
        "        self.report_manager = report_manager\n",
        "\n",
        "        self.loss = torch.nn.BCELoss(reduction='none')\n",
        "        assert grad_accum_count > 0\n",
        "        # Set model in training mode.\n",
        "        if (model):\n",
        "            self.model.train()\n",
        "            \n",
        "    def summ(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
        "          \"\"\" Validate model.\n",
        "              valid_iter: validate data iterator\n",
        "          Returns:\n",
        "              :obj:`nmt.Statistics`: validation loss statistics\n",
        "          \"\"\"\n",
        "          # Set model in validating mode.\n",
        "          def _get_ngrams(n, text):\n",
        "              ngram_set = set()\n",
        "              text_length = len(text)\n",
        "              max_index_ngram_start = text_length - n\n",
        "              for i in range(max_index_ngram_start + 1):\n",
        "                  ngram_set.add(tuple(text[i:i + n]))\n",
        "              return ngram_set\n",
        "\n",
        "          def _block_tri(c, p):\n",
        "              tri_c = _get_ngrams(3, c.split())\n",
        "              for s in p:\n",
        "                  tri_s = _get_ngrams(3, s.split())\n",
        "                  if len(tri_c.intersection(tri_s))>0:\n",
        "                      return True\n",
        "              return False\n",
        "\n",
        "          if (not cal_lead and not cal_oracle):\n",
        "              self.model.eval()\n",
        "          stats = Statistics()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for batch in test_iter:\n",
        "                  src = batch.src\n",
        "                  labels = batch.labels\n",
        "                  segs = batch.segs\n",
        "                  clss = batch.clss\n",
        "                  mask = batch.mask\n",
        "                  mask_cls = batch.mask_cls\n",
        "\n",
        "                  if (cal_lead):\n",
        "                      selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
        "                  elif (cal_oracle):\n",
        "                      selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
        "                                      range(batch.batch_size)]\n",
        "                  else:\n",
        "                      sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "                      sent_scores = sent_scores + mask.float()\n",
        "                      sent_scores = sent_scores.cpu().data.numpy()\n",
        "                      selected_ids = np.argsort(-sent_scores, 1)\n",
        "          return selected_ids\n",
        "\n",
        "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
        "                               report_stats):\n",
        "        if self.grad_accum_count > 1:\n",
        "            self.model.zero_grad()\n",
        "\n",
        "        for batch in true_batchs:\n",
        "            if self.grad_accum_count == 1:\n",
        "                self.model.zero_grad()\n",
        "\n",
        "            src = batch.src\n",
        "            labels = batch.labels\n",
        "            segs = batch.segs\n",
        "            clss = batch.clss\n",
        "            mask = batch.mask\n",
        "            mask_cls = batch.mask_cls\n",
        "\n",
        "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "            loss = self.loss(sent_scores, labels.float())\n",
        "            loss = (loss*mask.float()).sum()\n",
        "            (loss/loss.numel()).backward()\n",
        "            # loss.div(float(normalization)).backward()\n",
        "\n",
        "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
        "\n",
        "\n",
        "            total_stats.update(batch_stats)\n",
        "            report_stats.update(batch_stats)\n",
        "\n",
        "            # 4. Update the parameters and statistics.\n",
        "            if self.grad_accum_count == 1:\n",
        "                # Multi GPU gradient gather\n",
        "                if self.n_gpu > 1:\n",
        "                    grads = [p.grad.data for p in self.model.parameters()\n",
        "                             if p.requires_grad\n",
        "                             and p.grad is not None]\n",
        "                    distributed.all_reduce_and_rescale_tensors(\n",
        "                        grads, float(1))\n",
        "                self.optim.step()\n",
        "\n",
        "        # in case of multi step gradient accumulation,\n",
        "        # update only after accum batches\n",
        "        if self.grad_accum_count > 1:\n",
        "            if self.n_gpu > 1:\n",
        "                grads = [p.grad.data for p in self.model.parameters()\n",
        "                         if p.requires_grad\n",
        "                         and p.grad is not None]\n",
        "                distributed.all_reduce_and_rescale_tensors(\n",
        "                    grads, float(1))\n",
        "            self.optim.step()\n",
        "            \n",
        "    def _save(self, step):\n",
        "        real_model = self.model\n",
        "        # real_generator = (self.generator.module\n",
        "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
        "        #                   else self.generator)\n",
        "\n",
        "        model_state_dict = real_model.state_dict()\n",
        "        # generator_state_dict = real_generator.state_dict()\n",
        "        checkpoint = {\n",
        "            'model': model_state_dict,\n",
        "            # 'generator': generator_state_dict,\n",
        "            'opt': self.args,\n",
        "            'optim': self.optim,\n",
        "        }\n",
        "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
        "        logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
        "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
        "        if (not os.path.exists(checkpoint_path)):\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            return checkpoint, checkpoint_path\n",
        "\n",
        "    def _start_report_manager(self, start_time=None):\n",
        "        \"\"\"\n",
        "        Simple function to start report manager (if any)\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            if start_time is None:\n",
        "                self.report_manager.start()\n",
        "            else:\n",
        "                self.report_manager.start_time = start_time\n",
        "\n",
        "    def _maybe_gather_stats(self, stat):\n",
        "        \"\"\"\n",
        "        Gather statistics in multi-processes cases\n",
        "\n",
        "        Args:\n",
        "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
        "                or None (it returns None in this case)\n",
        "\n",
        "        Returns:\n",
        "            stat: the updated (or unchanged) stat object\n",
        "        \"\"\"\n",
        "        if stat is not None and self.n_gpu > 1:\n",
        "            return Statistics.all_gather_stats(stat)\n",
        "        return stat\n",
        "\n",
        "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
        "                               report_stats):\n",
        "        \"\"\"\n",
        "        Simple function to report training stats (if report_manager is set)\n",
        "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            return self.report_manager.report_training(\n",
        "                step, num_steps, learning_rate, report_stats,\n",
        "                multigpu=self.n_gpu > 1)\n",
        "        \n",
        "    def _report_step(self, learning_rate, step, train_stats=None,\n",
        "                     valid_stats=None):\n",
        "        \"\"\"\n",
        "        Simple function to report stats (if report_manager is set)\n",
        "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            return self.report_manager.report_step(\n",
        "                learning_rate, step, train_stats=train_stats,\n",
        "                valid_stats=valid_stats)\n",
        "\n",
        "    def _maybe_save(self, step):\n",
        "        \"\"\"\n",
        "        Save the model if a model saver is set\n",
        "        \"\"\"\n",
        "        if self.model_saver is not None:\n",
        "            self.model_saver.maybe_save(step)\n",
        "\n",
        "class BertData():\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
        "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
        "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
        "\n",
        "    def preprocess(self, src):\n",
        "\n",
        "        if (len(src) == 0):\n",
        "            return None\n",
        "\n",
        "        original_src_txt = [' '.join(s) for s in src]\n",
        "        idxs = [i for i, s in enumerate(src) if (len(s) > 1)]\n",
        "\n",
        "        src = [src[i][:2000] for i in idxs]\n",
        "        src = src[:1000]\n",
        "\n",
        "        if (len(src) < 3):\n",
        "            return None\n",
        "\n",
        "        src_txt = [' '.join(sent) for sent in src]\n",
        "        text = ' [SEP] [CLS] '.join(src_txt)\n",
        "        src_subtokens = self.tokenizer.tokenize(text)\n",
        "        src_subtokens = src_subtokens[:510]\n",
        "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
        "\n",
        "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
        "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
        "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
        "        segments_ids = []\n",
        "        for i, s in enumerate(segs):\n",
        "            if (i % 2 == 0):\n",
        "                segments_ids += s * [0]\n",
        "            else:\n",
        "                segments_ids += s * [1]\n",
        "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
        "        labels = None\n",
        "        src_txt = [original_src_txt[i] for i in idxs]\n",
        "        tgt_txt = None\n",
        "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
        "    \n",
        "def _lazy_dataset_loader(pt_file):\n",
        "  yield  pt_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = easydict.EasyDict({\n",
        "    \"encoder\":'classifier',\n",
        "    \"mode\":'test',\n",
        "    \"bert_data_path\":'../bert_data/korean',\n",
        "    \"model_path\":'../models/bert_classifier',\n",
        "    \"result_path\":'../results',\n",
        "    \"temp_dir\":'../temp',\n",
        "    \"batch_size\":1000,\n",
        "    \"use_interval\":True,\n",
        "    \"hidden_size\":128,\n",
        "    \"ff_size\":512,\n",
        "    \"heads\":4,\n",
        "    \"inter_layers\":2,\n",
        "    \"rnn_size\":512,\n",
        "    \"param_init\":0,\n",
        "    \"param_init_glorot\":True,\n",
        "    \"dropout\":0.1,\n",
        "    \"optim\":'adam',\n",
        "    \"lr\":2e-3,\n",
        "    \"report_every\":1,\n",
        "    \"save_checkpoint_steps\":5,\n",
        "    \"block_trigram\":True,\n",
        "    \"recall_eval\":False,\n",
        "    \n",
        "    \"accum_count\":1,\n",
        "    \"world_size\":1,\n",
        "    \"visible_gpus\":'-1', # cpu\n",
        "    \"gpu_ranks\":'0',\n",
        "    \"log_file\":'../logs/train.txt',\n",
        "    \"test_from\":'../models/bert_classifier/model_step_25000.pt'\n",
        "})\n",
        "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test(args, input_data, -1, '', None)\n",
        "pt = ''\n",
        "step = None\n",
        "\n",
        "init_logger(args.log_file)\n",
        "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "device_id = 0 if device == \"cuda\" else -1\n",
        "\n",
        "cp = args.test_from\n",
        "try:\n",
        "    step = int(cp.split('.')[-2].split('_')[-1])\n",
        "except:\n",
        "    step = 0\n",
        "\n",
        "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "if (pt != ''):\n",
        "    test_from = pt\n",
        "else:\n",
        "    test_from = args.test_from\n",
        "logger.info('Loading checkpoint from %s' % test_from)\n",
        "checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n",
        "opt = vars(checkpoint['opt'])\n",
        "for k in opt.keys():\n",
        "    if (k in model_flags):\n",
        "        setattr(args, k, opt[k])\n",
        "\n",
        "config = BertConfig.from_pretrained('bert-base-multilingual-cased')\n",
        "model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
        "model.load_cp(checkpoint)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def test(args, input_list, device_id, pt, step):\n",
        "def test(args, input_list):\n",
        "  test_iter = data_loader.Dataloader(args, _lazy_dataset_loader(input_list),\n",
        "                                args.batch_size, device,\n",
        "                                shuffle=False, is_test=True)\n",
        "  trainer = build_trainer(args, device_id, model, None)\n",
        "  result = trainer.summ(test_iter, step)\n",
        "  return result, input_list\n",
        "\n",
        "# args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def txt2input(text):\n",
        "    data = list(filter(None, text.split('\\n')))\n",
        "    bertdata = BertData()\n",
        "    txt_data = bertdata.preprocess(data)\n",
        "    data_dict = {\"src\":txt_data[0],\n",
        "               \"labels\":[0,1,2],\n",
        "               \"segs\":txt_data[2],\n",
        "               \"clss\":txt_data[3],\n",
        "               \"src_txt\":txt_data[4],\n",
        "               \"tgt_txt\":None}\n",
        "    input_data = []\n",
        "    input_data.append(data_dict)\n",
        "    return input_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_covid = pd.read_csv(\"../한겨레_코로나_content.csv\")\n",
        "df_covid.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "idxs = list(range(27237))\n",
        "texts = df_covid['content'].tolist()\n",
        "idx_text = list(zip(idxs, texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "for i in tqdm(range(len(idx_text))):\n",
        "    idx_text[i] = list(idx_text[i])\n",
        "    if (idx_text[i][1] != \"\"):\n",
        "        if (isinstance(idx_text[i][1], str)):\n",
        "            text_list = idx_text[i][1].split('\\n')\n",
        "            re_text = ''\n",
        "\n",
        "            for text in (text_list):\n",
        "                if len(text) > 50:\n",
        "                    text = re.sub(r\"([\\w\\.-]+)@([\\w\\.-]+)(\\.[\\w\\.]+)\", \"\", text) # 이메일 검사\n",
        "\n",
        "                    if \" 기자\" in text: # 기자 검사\n",
        "                        repoter_check = text.split(\" \")\n",
        "                        while \"기자\" in repoter_check:\n",
        "                            index = repoter_check.index(\"기자\")\n",
        "                            del repoter_check[index]\n",
        "                            del repoter_check[index-1]\n",
        "                        text = ' '.join(r for r in repoter_check)\n",
        "\n",
        "                    re_text += (text + '\\n')\n",
        "\n",
        "            idx_text[i][1] = re_text\n",
        "        \n",
        "    else:\n",
        "        idx_text[i][1] = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def inference(texts):\n",
        "    global idx_text, df_covid\n",
        "    time.sleep(1)\n",
        "    idx = texts[0]\n",
        "    text = texts[1]\n",
        "    if text == \"\" or pd.isna(text):\n",
        "        new_list.append(\n",
        "            {\n",
        "                \"index\" : idx,\n",
        "                \"summary\" : \"\"\n",
        "            }\n",
        "        )\n",
        "        df_covid = df_covid.drop(idx, axis=0)\n",
        "#         idx_text.remove(texts)\n",
        "        return ''\n",
        "    else:\n",
        "        if len(text.split('\\n')) <= 3: # 원문 기사가 짧은 경우. txt2input에서 none 타입이 됨\n",
        "            new_list.append(\n",
        "                    {\n",
        "                        \"index\" : idx,\n",
        "                        \"summary\" : text\n",
        "                    }\n",
        "            )\n",
        "        else:\n",
        "            input_data = txt2input(text)\n",
        "            sum_list = test(args, input_data)\n",
        "            result = [list(filter(None, text.split('\\n')))[i] for i in sum_list[0][0][:2]]\n",
        "            summary = (result[0] + \" \" + result[1])\n",
        "            new_list.append(\n",
        "                    {\n",
        "                        \"index\" : idx,\n",
        "                        \"summary\" : summary\n",
        "                    }\n",
        "            )\n",
        "        df_covid = df_covid.drop(idx, axis=0)\n",
        "#         idx_text.remove(texts)\n",
        "        return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pool = ThreadPool(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_list = []\n",
        "\n",
        "try:\n",
        "    for _ in tqdm(pool.imap_unordered(inference, idx_text[0:1000]), total=len(idx_text[0:1000])):\n",
        "        pass\n",
        "except RuntimeError:\n",
        "    pool.close()\n",
        "    pool.terminate()\n",
        "    pool.join()\n",
        "    \n",
        "    df_covid.to_csv(\"../한겨레_코로나_content_1.csv\")\n",
        "    \n",
        "    col_name = [\"index\", \"summary\"]\n",
        "    news_df = pd.DataFrame(new_list, columns=col_name)\n",
        "\n",
        "    csv_file_name = \"../한겨레_covid_summary_1.csv\"\n",
        "\n",
        "    news_df.to_csv(csv_file_name)\n",
        "    news_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text = '''\n",
        "# 일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다.\n",
        "\n",
        "# 정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다.\n",
        "\n",
        "# 앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다.\n",
        "\n",
        "# 한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다.\n",
        "\n",
        "# 가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 ㅌ사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다.\n",
        "\n",
        "# 그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다.\n",
        "\n",
        "# 가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다.\n",
        "\n",
        "# 그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다.\n",
        "# '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input_data = txt2input(text)\n",
        "# sum_list = test(args, input_data)\n",
        "# sum_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [list(filter(None, text.split('\\n')))[i] for i in sum_list[0][0][:3]][0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "BertSum Test",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
