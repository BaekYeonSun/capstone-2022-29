{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c27a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install g++ openjdk-8-jdk \n",
    "# !pip3 install konlpy JPype1-py3\n",
    "# !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40caccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "mecab.morphs('동해물과 백두산이 마르고 닳도록')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorboardx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e78ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyrouge --upgrade\n",
    "# !pip install https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
    "# !pip install pyrouge\n",
    "# !pip show pyrouge\n",
    "# !git clone https://github.com/andersjo/pyrouge.git\n",
    "from pyrouge import Rouge155\n",
    "# !pyrouge_set_rouge_path 'pyrouge/tools/ROUGE-1.5.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85038c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -y libxml-parser-perl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac99042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%shell\n",
    "# !cd pyrouge/tools/ROUGE-1.5.5/data\n",
    "# !rm WordNet-2.0.exc.db # only if exist\n",
    "# !cd WordNet-2.0-Exceptions\n",
    "# !rm WordNet-2.0.exc.db # only if exist\n",
    "\n",
    "# !./buildExeptionDB.pl . exc WordNet-2.0.exc.db\n",
    "# !cd ../\n",
    "# !ln -s WordNet-2.0-Exceptions/WordNet-2.0.exc.db WordNet-2.0.exc.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content\n",
    "# !git clone https://github.com/HaloKim/KorBertSum.git\n",
    "# %cd /content/KorBertSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a777590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "train = pd.read_csv('json_to_csv_train_신문기사.csv').drop('Unnamed: 0', axis=1)\n",
    "print(len(train))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf18eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv('json_to_csv_valid_신문기사.csv').drop('Unnamed: 0', axis=1)\n",
    "print(len(valid))\n",
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa89ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train,valid])\n",
    "print(len(all_data))\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a0e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set = train\n",
    "valid_set, test_set = train_test_split(valid, test_size = 0.1)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_set, test_set = train_test_split(all_data, test_size = 0.4)\n",
    "# valid_set, test_set = train_test_split(test_set, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set))\n",
    "print(len(valid_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb96351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "list_dic = []\n",
    "for idx, row in train_set.iterrows():\n",
    "  raw = row['article_orginal']\n",
    "  target_idx = ast.literal_eval(row['extractive'])\n",
    "\n",
    "  sentences = raw.split(',')\n",
    "  src = [i.split(' ') for i in sentences]\n",
    "  tgt = [a for i,a in enumerate(src) if i in target_idx]\n",
    "  \n",
    "  mydict = {}\n",
    "  mydict['src'] = src\n",
    "  mydict['tgt'] = tgt\n",
    "  list_dic.append(mydict)\n",
    "        \n",
    "temp = []\n",
    "for i,a in enumerate(tqdm(list_dic)):\n",
    "  if (i+1)%1000!=0:\n",
    "      temp.append(a)\n",
    "  else:\n",
    "      filename = 'korean.'+'train'+'.'+str(i//1000)+'.json'\n",
    "      with open('KorBertSum/json/'+filename, \"w\", encoding='utf-8') as json_file:\n",
    "          json.dump(temp, json_file, ensure_ascii=False)\n",
    "      temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ce949",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  os.mkdir('KorBertSum/json/val')\n",
    "except:\n",
    "  pass\n",
    "\n",
    "list_dic = []\n",
    "for idx, row in valid_set.iterrows():\n",
    "        raw = row['article_orginal']\n",
    "        target_idx = ast.literal_eval(row['extractive'])\n",
    "\n",
    "        sentences = raw.split(',')\n",
    "        src = [i.split(' ') for i in sentences]\n",
    "        tgt = [a for i,a in enumerate(src) if i in target_idx]\n",
    "        \n",
    "        mydict = {}\n",
    "        mydict['src'] = src\n",
    "        mydict['tgt'] = tgt\n",
    "        list_dic.append(mydict)\n",
    "        \n",
    "temp = []\n",
    "for i,a in enumerate(tqdm(list_dic)):\n",
    "    \n",
    "    if (i+1)%1000!=0:\n",
    "        temp.append(a)\n",
    "    else:\n",
    "        filename = 'korean.'+'valid'+'.'+str(i//1000)+'.json'\n",
    "        with open('KorBertSum/json/val/'+filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(temp, json_file, ensure_ascii=False)\n",
    "        temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  os.mkdir('KorBertSum/json/test')\n",
    "except:\n",
    "  pass\n",
    "\n",
    "list_dic = []\n",
    "for idx, row in test_set.iterrows():\n",
    "        raw = row['article_orginal']\n",
    "        target_idx = ast.literal_eval(row['extractive'])\n",
    "\n",
    "        sentences = raw.split(',')\n",
    "        src = [i.split(' ') for i in sentences]\n",
    "        tgt = [a for i,a in enumerate(src) if i in target_idx]\n",
    "        \n",
    "        mydict = {}\n",
    "        mydict['src'] = src\n",
    "        mydict['tgt'] = tgt\n",
    "        list_dic.append(mydict)\n",
    "        \n",
    "temp = []\n",
    "for i,a in enumerate(tqdm(list_dic)):\n",
    "    \n",
    "    if (i+1)%1000!=0:\n",
    "        temp.append(a)\n",
    "    else:\n",
    "        filename = 'korean.'+'test'+'.'+str(i//1000)+'.json'\n",
    "        with open('KorBertSum/json/test/'+filename, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(temp, json_file, ensure_ascii=False)\n",
    "        temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 처리모드, json경로, 저장경로, 로그파일\n",
    "!python KorBertSum/src/preprocess.py \\\n",
    "  -mode format_to_bert \\\n",
    "  -raw_path KorBertSum/json \\\n",
    "  -save_path KorBertSum/bert_data \\\n",
    "  -dataset train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8867f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  os.mkdir('KorBertSum/bert_data/test')\n",
    "except:\n",
    "  pass\n",
    "\n",
    "!python KorBertSum/src/preprocess.py \\\n",
    "  -mode format_to_bert \\\n",
    "  -raw_path KorBertSum/json/val \\\n",
    "  -save_path KorBertSum/bert_data/test \\\n",
    "  -dataset valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1899ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  os.mkdir('KorBertSum/bert_data/test')\n",
    "except:\n",
    "  pass\n",
    "\n",
    "!python KorBertSum/src/preprocess.py \\\n",
    "  -mode format_to_bert \\\n",
    "  -raw_path KorBertSum/json/test \\\n",
    "  -save_path KorBertSum/bert_data/test \\\n",
    "  -dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d284f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python KorBertSum/src/train.py \\\n",
    "  -mode train \\\n",
    "  -encoder transformer \\\n",
    "  -bert_data_path KorBertSum/bert_data/korean \\\n",
    "  -model_path KorBertSum/models/bert_classifier_3 \\\n",
    "  -log_file KorBertSum/logs/train_3.txt \\\n",
    "  -lr 0.002 -visible_gpus 0 -gpu_ranks 0 -world_size 1 -report_every 1000 -dropout 0.5 \\\n",
    "  -save_checkpoint_steps 5000 -batch_size 3000 -decay_method noam -train_steps 65000 -accum_count 1 \\\n",
    "  -optim adamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python KorBertSum/src/train.py \\\n",
    "  -mode validate \\\n",
    "  -bert_data_path KorBertSum/bert_data/test/korean \\\n",
    "  -model_path KorBertSum/models/bert_trans_2 \\\n",
    "  -visible_gpus 0 -gpu_ranks 0 -batch_size 3000 \\\n",
    "  -log_file KorBertSum/logs/val_trans_2.txt \\\n",
    "  -temp_dir KorBertSum/temp \\\n",
    "  -result_path KorBertSum/results \\\n",
    "  -lr 0.002 \\\n",
    "#   -test_all 1 \\\n",
    "#   -test_from KorBertSum/models/bert_classifier/model_step_30000.pt \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83edf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python KorBertSum/src/train.py \\\n",
    "  -mode test \\\n",
    "  -bert_data_path KorBertSum/bert_data/test/korean \\\n",
    "  -visible_gpus 0 -gpu_ranks 0 -batch_size 3000 \\\n",
    "  -log_file KorBertSum/logs/test.txt \\\n",
    "  -temp_dir KorBertSum/temp \\\n",
    "  -result_path KorBertSum/results \\\n",
    "  -test_from KorBertSum/models/bert_classifier_3/model_step_65000.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Notebook)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
