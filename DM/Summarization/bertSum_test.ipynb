{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb1f4f5",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e82c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyrouge --upgrade\n",
    "# !pip install https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
    "# !pip install pyrouge\n",
    "# !pip show pyrouge\n",
    "# !git clone https://github.com/andersjo/pyrouge.git\n",
    "# from pyrouge import Rouge155\n",
    "# !pyrouge_set_rouge_path 'pyrouge/tools/ROUGE-1.5.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e86e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install tensorboardX\n",
    "# !pip install easydict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe22622c",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fae25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('KorBertSum/src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from models import data_loader, model_builder\n",
    "from models.model_builder import Summarizer\n",
    "from others.logging import logger, init_logger\n",
    "from models.data_loader import load_dataset\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from tensorboardX import SummaryWriter\n",
    "from models.reporter import ReportMgr\n",
    "from models.stats import Statistics\n",
    "import easydict\n",
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a26efed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tally_parameters(model):\n",
    "    n_params = sum([p.nelement() for p in model.parameters()])\n",
    "    return n_params\n",
    "\n",
    "def build_trainer(args, device_id, model,\n",
    "                  optim):\n",
    "    \"\"\"\n",
    "    Simplify `Trainer` creation based on user `opt`s*\n",
    "    Args:\n",
    "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
    "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
    "        fields (dict): dict of fields\n",
    "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
    "        data_type (str): string describing the type of data\n",
    "            e.g. \"text\", \"img\", \"audio\"\n",
    "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
    "            used to save the model\n",
    "    \"\"\"\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "\n",
    "\n",
    "    grad_accum_count = args.accum_count\n",
    "    n_gpu = args.world_size\n",
    "\n",
    "    if device_id >= 0:\n",
    "        gpu_rank = int(args.gpu_ranks[device_id])\n",
    "    else:\n",
    "        gpu_rank = 0\n",
    "        n_gpu = 0\n",
    "\n",
    "    print('gpu_rank %d' % gpu_rank)\n",
    "\n",
    "    tensorboard_log_dir = args.model_path\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
    "\n",
    "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
    "\n",
    "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
    "\n",
    "    # print(tr)\n",
    "    if (model):\n",
    "        n_params = _tally_parameters(model)\n",
    "        logger.info('* number of parameters: %d' % n_params)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Class that controls the training process.\n",
    "\n",
    "    Args:\n",
    "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
    "                to train\n",
    "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
    "               the optimizer responsible for update\n",
    "            trunc_size(int): length of truncated back propagation through time\n",
    "            shard_size(int): compute loss in shards of this size for efficiency\n",
    "            data_type(string): type of the source input: [text|img|audio]\n",
    "            norm_method(string): normalization methods: [sents|tokens]\n",
    "            grad_accum_count(int): accumulate gradients this many times.\n",
    "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
    "                the object that creates reports, or None\n",
    "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
    "                used to save a checkpoint.\n",
    "                Thus nothing will be saved if this parameter is None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  args, model,  optim,\n",
    "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
    "                  report_manager=None):\n",
    "        # Basic attributes.\n",
    "        self.args = args\n",
    "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.grad_accum_count = grad_accum_count\n",
    "        self.n_gpu = n_gpu\n",
    "        self.gpu_rank = gpu_rank\n",
    "        self.report_manager = report_manager\n",
    "\n",
    "        self.loss = torch.nn.BCELoss(reduction='none')\n",
    "        assert grad_accum_count > 0\n",
    "        # Set model in training mode.\n",
    "        if (model):\n",
    "            self.model.train()\n",
    "            \n",
    "    def summ(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
    "          \"\"\" Validate model.\n",
    "              valid_iter: validate data iterator\n",
    "          Returns:\n",
    "              :obj:`nmt.Statistics`: validation loss statistics\n",
    "          \"\"\"\n",
    "          # Set model in validating mode.\n",
    "          def _get_ngrams(n, text):\n",
    "              ngram_set = set()\n",
    "              text_length = len(text)\n",
    "              max_index_ngram_start = text_length - n\n",
    "              for i in range(max_index_ngram_start + 1):\n",
    "                  ngram_set.add(tuple(text[i:i + n]))\n",
    "              return ngram_set\n",
    "\n",
    "          def _block_tri(c, p):\n",
    "              tri_c = _get_ngrams(3, c.split())\n",
    "              for s in p:\n",
    "                  tri_s = _get_ngrams(3, s.split())\n",
    "                  if len(tri_c.intersection(tri_s))>0:\n",
    "                      return True\n",
    "              return False\n",
    "\n",
    "          if (not cal_lead and not cal_oracle):\n",
    "              self.model.eval()\n",
    "          stats = Statistics()\n",
    "\n",
    "          with torch.no_grad():\n",
    "              for batch in test_iter:\n",
    "                  src = batch.src\n",
    "                  labels = batch.labels\n",
    "                  segs = batch.segs\n",
    "                  clss = batch.clss\n",
    "                  mask = batch.mask\n",
    "                  mask_cls = batch.mask_cls\n",
    "\n",
    "                  if (cal_lead):\n",
    "                      selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
    "                  elif (cal_oracle):\n",
    "                      selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
    "                                      range(batch.batch_size)]\n",
    "                  else:\n",
    "                      sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "                      sent_scores = sent_scores + mask.float()\n",
    "                      sent_scores = sent_scores.cpu().data.numpy()\n",
    "                      selected_ids = np.argsort(-sent_scores, 1)\n",
    "          return selected_ids\n",
    "\n",
    "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
    "                               report_stats):\n",
    "        if self.grad_accum_count > 1:\n",
    "            self.model.zero_grad()\n",
    "\n",
    "        for batch in true_batchs:\n",
    "            if self.grad_accum_count == 1:\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            src = batch.src\n",
    "            labels = batch.labels\n",
    "            segs = batch.segs\n",
    "            clss = batch.clss\n",
    "            mask = batch.mask\n",
    "            mask_cls = batch.mask_cls\n",
    "\n",
    "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "            loss = self.loss(sent_scores, labels.float())\n",
    "            loss = (loss*mask.float()).sum()\n",
    "            (loss/loss.numel()).backward()\n",
    "            # loss.div(float(normalization)).backward()\n",
    "\n",
    "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
    "\n",
    "\n",
    "            total_stats.update(batch_stats)\n",
    "            report_stats.update(batch_stats)\n",
    "\n",
    "            # 4. Update the parameters and statistics.\n",
    "            if self.grad_accum_count == 1:\n",
    "                # Multi GPU gradient gather\n",
    "                if self.n_gpu > 1:\n",
    "                    grads = [p.grad.data for p in self.model.parameters()\n",
    "                             if p.requires_grad\n",
    "                             and p.grad is not None]\n",
    "                    distributed.all_reduce_and_rescale_tensors(\n",
    "                        grads, float(1))\n",
    "                self.optim.step()\n",
    "\n",
    "        # in case of multi step gradient accumulation,\n",
    "        # update only after accum batches\n",
    "        if self.grad_accum_count > 1:\n",
    "            if self.n_gpu > 1:\n",
    "                grads = [p.grad.data for p in self.model.parameters()\n",
    "                         if p.requires_grad\n",
    "                         and p.grad is not None]\n",
    "                distributed.all_reduce_and_rescale_tensors(\n",
    "                    grads, float(1))\n",
    "            self.optim.step()\n",
    "            \n",
    "    def _save(self, step):\n",
    "        real_model = self.model\n",
    "        # real_generator = (self.generator.module\n",
    "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
    "        #                   else self.generator)\n",
    "\n",
    "        model_state_dict = real_model.state_dict()\n",
    "        # generator_state_dict = real_generator.state_dict()\n",
    "        checkpoint = {\n",
    "            'model': model_state_dict,\n",
    "            # 'generator': generator_state_dict,\n",
    "            'opt': self.args,\n",
    "            'optim': self.optim,\n",
    "        }\n",
    "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
    "        logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
    "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
    "        if (not os.path.exists(checkpoint_path)):\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            return checkpoint, checkpoint_path\n",
    "\n",
    "    def _start_report_manager(self, start_time=None):\n",
    "        \"\"\"\n",
    "        Simple function to start report manager (if any)\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            if start_time is None:\n",
    "                self.report_manager.start()\n",
    "            else:\n",
    "                self.report_manager.start_time = start_time\n",
    "\n",
    "    def _maybe_gather_stats(self, stat):\n",
    "        \"\"\"\n",
    "        Gather statistics in multi-processes cases\n",
    "\n",
    "        Args:\n",
    "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
    "                or None (it returns None in this case)\n",
    "\n",
    "        Returns:\n",
    "            stat: the updated (or unchanged) stat object\n",
    "        \"\"\"\n",
    "        if stat is not None and self.n_gpu > 1:\n",
    "            return Statistics.all_gather_stats(stat)\n",
    "        return stat\n",
    "\n",
    "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
    "                               report_stats):\n",
    "        \"\"\"\n",
    "        Simple function to report training stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_training(\n",
    "                step, num_steps, learning_rate, report_stats,\n",
    "                multigpu=self.n_gpu > 1)\n",
    "        \n",
    "    def _report_step(self, learning_rate, step, train_stats=None,\n",
    "                     valid_stats=None):\n",
    "        \"\"\"\n",
    "        Simple function to report stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_step(\n",
    "                learning_rate, step, train_stats=train_stats,\n",
    "                valid_stats=valid_stats)\n",
    "\n",
    "    def _maybe_save(self, step):\n",
    "        \"\"\"\n",
    "        Save the model if a model saver is set\n",
    "        \"\"\"\n",
    "        if self.model_saver is not None:\n",
    "            self.model_saver.maybe_save(step)\n",
    "\n",
    "class BertData():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
    "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
    "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
    "\n",
    "    def preprocess(self, src):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > 1)]\n",
    "\n",
    "        src = [src[i][:2000] for i in idxs]\n",
    "        src = src[:1000]\n",
    "\n",
    "        if (len(src) < 3):\n",
    "            return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "        src_subtokens = src_subtokens[:510]\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        labels = None\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        tgt_txt = None\n",
    "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
    "    \n",
    "def _lazy_dataset_loader(pt_file):\n",
    "  yield  pt_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7eb39",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d7256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"encoder\":'classifier',\n",
    "    \"mode\":'test',\n",
    "    \"bert_data_path\":'../bert_data/korean',\n",
    "    \"model_path\":'../models/bert_classifier',\n",
    "    \"result_path\":'../results',\n",
    "    \"temp_dir\":'../temp',\n",
    "    \"batch_size\":1000,\n",
    "    \"use_interval\":True,\n",
    "    \"hidden_size\":128,\n",
    "    \"ff_size\":512,\n",
    "    \"heads\":4,\n",
    "    \"inter_layers\":2,\n",
    "    \"rnn_size\":512,\n",
    "    \"param_init\":0,\n",
    "    \"param_init_glorot\":True,\n",
    "    \"dropout\":0.1,\n",
    "    \"optim\":'adam',\n",
    "    \"lr\":2e-3,\n",
    "    \"report_every\":1,\n",
    "    \"save_checkpoint_steps\":100,\n",
    "    \"block_trigram\":True,\n",
    "    \"recall_eval\":False,\n",
    "    \n",
    "    \"accum_count\":1,\n",
    "    \"world_size\":1,\n",
    "    \"visible_gpus\":'0', # cpu\n",
    "    \"gpu_ranks\":'0',\n",
    "    \"log_file\":'../logs/train.txt',\n",
    "    \"test_from\":'../models/bert_classifier/model_step_65000.pt'\n",
    "})\n",
    "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# Model Load\n",
    "# test(args, input_data, -1, '', None)\n",
    "pt = ''\n",
    "step = None\n",
    "\n",
    "init_logger(args.log_file)\n",
    "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "device_id = 0 if device == \"cuda\" else -1\n",
    "\n",
    "cp = args.test_from\n",
    "try:\n",
    "    step = int(cp.split('.')[-2].split('_')[-1])\n",
    "except:\n",
    "    step = 0\n",
    "\n",
    "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "if (pt != ''):\n",
    "    test_from = pt\n",
    "else:\n",
    "    test_from = args.test_from\n",
    "logger.info('Loading checkpoint from %s' % test_from)\n",
    "checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n",
    "opt = vars(checkpoint['opt'])\n",
    "for k in opt.keys():\n",
    "    if (k in model_flags):\n",
    "        setattr(args, k, opt[k])\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-multilingual-cased')\n",
    "model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
    "model.load_cp(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5301c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, input_list):\n",
    "  test_iter = data_loader.Dataloader(args, _lazy_dataset_loader(input_list),\n",
    "                                args.batch_size, device,\n",
    "                                shuffle=False, is_test=True)\n",
    "  trainer = build_trainer(args, device_id, model, None)\n",
    "  result = trainer.summ(test_iter, step)\n",
    "  return result, input_list\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "def txt2input(text):\n",
    "  data = list(filter(None, text.split('\\n')))\n",
    "  bertdata = BertData()\n",
    "  txt_data = bertdata.preprocess(data)\n",
    "  data_dict = {\"src\":txt_data[0],\n",
    "               \"labels\":[0,1,2],\n",
    "               \"segs\":txt_data[2],\n",
    "               \"clss\":txt_data[3],\n",
    "               \"src_txt\":txt_data[4],\n",
    "               \"tgt_txt\":None}\n",
    "  input_data = []\n",
    "  input_data.append(data_dict)\n",
    "  return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14476e7",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e790bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-14 23:38:09,769 INFO] Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“8번째 확진자 군산 대중목욕탕·대형마트 등 방문”</td>\n",
       "      <td>전북도, 방문한 내과 병원은 임시 휴업\\n\\n접촉자 72명에 대해 능동감시체계 돌입...</td>\n",
       "      <td>2020.2.2</td>\n",
       "      <td>오후 4시 50분 28초</td>\n",
       "      <td>https://www.hani.co.kr/arti/area/honam/926579....</td>\n",
       "      <td>26764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[세상읽기] 내가 그들의 사회안전망이다 / 류영재</td>\n",
       "      <td>1994년 르완다에서 다수민족인 후투족이 소수민족인 투치족을 학살하는 사태가 벌어졌...</td>\n",
       "      <td>2020.2.2</td>\n",
       "      <td>오후 6시 51분 55초</td>\n",
       "      <td>https://www.hani.co.kr/arti/opinion/column/926...</td>\n",
       "      <td>26770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11일만에 문 여는 중국 증시 9% 대폭락?</td>\n",
       "      <td>휴장기간 거래된 중국지수 ETF가 가늠자\\n\\n뉴욕증시 상장된 CSI300 ETF ...</td>\n",
       "      <td>2020.2.2</td>\n",
       "      <td>오후 6시 58분 43초</td>\n",
       "      <td>https://www.hani.co.kr/arti/economy/finance/92...</td>\n",
       "      <td>26771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2월 3일 알림</td>\n",
       "      <td>이해준 현대무용협회장\\n\\n◇ 한국현대무용협회는 지난달 31일 서울 대학로 한국장애...</td>\n",
       "      <td>2020.2.2</td>\n",
       "      <td>오후 7시 7분 11초</td>\n",
       "      <td>https://www.hani.co.kr/arti/society/ngo/926602...</td>\n",
       "      <td>26772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>손끝까지 꼼꼼히, 마스크 철벽…‘셀프 방역’이 가장 확실한 방역</td>\n",
       "      <td>지역사회 감염 불안감 커지지만\\n\\n위생수칙만 지켜도 피할 수 있어\\n\\n마스크 착...</td>\n",
       "      <td>2020.2.2</td>\n",
       "      <td>오후 8시 26분 10초</td>\n",
       "      <td>https://www.hani.co.kr/arti/economy/finance/92...</td>\n",
       "      <td>26775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title  \\\n",
       "0         “8번째 확진자 군산 대중목욕탕·대형마트 등 방문”   \n",
       "1          [세상읽기] 내가 그들의 사회안전망이다 / 류영재   \n",
       "2             11일만에 문 여는 중국 증시 9% 대폭락?   \n",
       "3                             2월 3일 알림   \n",
       "4  손끝까지 꼼꼼히, 마스크 철벽…‘셀프 방역’이 가장 확실한 방역   \n",
       "\n",
       "                                             content      date           time  \\\n",
       "0  전북도, 방문한 내과 병원은 임시 휴업\\n\\n접촉자 72명에 대해 능동감시체계 돌입...  2020.2.2  오후 4시 50분 28초   \n",
       "1  1994년 르완다에서 다수민족인 후투족이 소수민족인 투치족을 학살하는 사태가 벌어졌...  2020.2.2  오후 6시 51분 55초   \n",
       "2  휴장기간 거래된 중국지수 ETF가 가늠자\\n\\n뉴욕증시 상장된 CSI300 ETF ...  2020.2.2  오후 6시 58분 43초   \n",
       "3  이해준 현대무용협회장\\n\\n◇ 한국현대무용협회는 지난달 31일 서울 대학로 한국장애...  2020.2.2   오후 7시 7분 11초   \n",
       "4  지역사회 감염 불안감 커지지만\\n\\n위생수칙만 지켜도 피할 수 있어\\n\\n마스크 착...  2020.2.2  오후 8시 26분 10초   \n",
       "\n",
       "                                                 url  index  \n",
       "0  https://www.hani.co.kr/arti/area/honam/926579....  26764  \n",
       "1  https://www.hani.co.kr/arti/opinion/column/926...  26770  \n",
       "2  https://www.hani.co.kr/arti/economy/finance/92...  26771  \n",
       "3  https://www.hani.co.kr/arti/society/ngo/926602...  26772  \n",
       "4  https://www.hani.co.kr/arti/economy/finance/92...  26775  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "### 입력 csv 파일 바꿔야함\n",
    "df_covid = pd.read_csv(\"../test_content_f_54.csv\").drop('Unnamed: 0', axis=1)\n",
    "# df_covid.head()\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# index 열 추가\n",
    "# idxs = list(range(27251))\n",
    "# df_covid['index'] = idxs\n",
    "# df_covid.head()\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# 리스트 생성\n",
    "# idxs = df_covid['Unnamed: 0'].tolist()\n",
    "idxs = df_covid['index'].tolist()\n",
    "texts = df_covid['content'].tolist()\n",
    "titles = df_covid['title'].tolist()\n",
    "dates = df_covid['date'].tolist()\n",
    "urls = df_covid['url'].tolist()\n",
    "idx_text = list(zip(idxs, texts, titles, dates, urls))\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# # 기준 정하기\n",
    "# print(len('박임근 기자 hanjeoung990111@kookmin.ac.kr'))\n",
    "# print(len('지난해 3월 만경강 도보여행길 걷기에 앞서 발원지인 밤샘에서 촬영한 모습. 박영환씨 제공'))\n",
    "# 논문에서 50 미만 제거 함\n",
    "\n",
    "# 문자열 길이 확인 및 제거 & 기자 및 이메일 제거\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "for i in tqdm(range(len(idx_text))):\n",
    "    idx_text[i] = list(idx_text[i])\n",
    "    if (idx_text[i][1] != \"\"):\n",
    "        if (isinstance(idx_text[i][1], str)):\n",
    "            text_list = idx_text[i][1].split('\\n')\n",
    "            re_text = ''\n",
    "\n",
    "            for text in (text_list):\n",
    "                if len(text) > 50:\n",
    "                    text = re.sub(r\"([\\w\\.-]+)@([\\w\\.-]+)(\\.[\\w\\.]+)\", \"\", text) # 이메일 검사\n",
    "\n",
    "                    if \" 기자\" in text: # 기자 검사\n",
    "                        repoter_check = text.split(\" \")\n",
    "                        while \"기자\" in repoter_check:\n",
    "                            index = repoter_check.index(\"기자\")\n",
    "                            del repoter_check[index]\n",
    "                            del repoter_check[index-1]\n",
    "                        text = ' '.join(r for r in repoter_check)\n",
    "\n",
    "                    re_text += (text + '\\n')\n",
    "\n",
    "            idx_text[i][1] = re_text\n",
    "        \n",
    "    else:\n",
    "        idx_text[i][1] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd4830",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dda777b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def inference(texts):\n",
    "    global df_covid\n",
    "    time.sleep(1)\n",
    "    idx = texts[0]\n",
    "    text = texts[1]\n",
    "    title = texts[2]\n",
    "    date = texts[3]\n",
    "    url = texts[4]\n",
    "    df = df_covid[df_covid['index'] == idx]\n",
    "    index = df.index[0]\n",
    "    if text == \"\" or pd.isna(text):\n",
    "        new_list.append(\n",
    "            {\n",
    "                \"index\" : idx,\n",
    "                \"title\" : title,\n",
    "                \"content\" : text,\n",
    "                \"date\" : date,\n",
    "                \"url\" : url,\n",
    "                \"summary\" : \"\"\n",
    "            }\n",
    "        )\n",
    "        df_covid = df_covid.drop(index, axis=0)\n",
    "    else:\n",
    "        if len(text.split('\\n')) <= 3: # 원문 기사가 짧은 경우. txt2input에서 none 타입이 됨\n",
    "            new_list.append(\n",
    "                    {\n",
    "                        \"index\" : idx,\n",
    "                        \"title\" : title,\n",
    "                        \"content\" : text,\n",
    "                        \"date\" : date,\n",
    "                        \"url\" : url,\n",
    "                        \"summary\" : text\n",
    "                    }\n",
    "            )\n",
    "        else:\n",
    "            input_data = txt2input(text)\n",
    "            sum_list = test(args, input_data)\n",
    "            result = [list(filter(None, text.split('\\n')))[i] for i in sum_list[0][0][:2]]\n",
    "            try:\n",
    "                summary = (result[0] + \" \" + result[1])\n",
    "                new_list.append(\n",
    "                        {\n",
    "                            \"index\" : idx,\n",
    "                            \"title\" : title,\n",
    "                            \"content\" : text,\n",
    "                            \"date\" : date,\n",
    "                            \"url\" : url,\n",
    "                            \"summary\" : summary\n",
    "                        }\n",
    "                )\n",
    "            except:\n",
    "                summary = (result[0])\n",
    "                new_list.append(\n",
    "                        {\n",
    "                            \"index\" : idx,\n",
    "                            \"title\" : title,\n",
    "                            \"content\" : text,\n",
    "                            \"date\" : date,\n",
    "                            \"url\" : url,\n",
    "                            \"summary\" : summary\n",
    "                        }\n",
    "                )\n",
    "        df_covid = df_covid.drop(index, axis=0)\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cdb65f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n",
      "gpu_rank 0\n",
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-14 23:38:19,099 INFO] * number of parameters: 177854209\n",
      "[2022-05-14 23:38:19,310 INFO] * number of parameters: 177854209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n",
      "gpu_rank 0\n",
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-14 23:38:19,330 INFO] * number of parameters: 177854209\n",
      "[2022-05-14 23:38:19,663 INFO] * number of parameters: 177854209\n",
      "[2022-05-14 23:38:19,682 INFO] * number of parameters: 177854209\n",
      "[2022-05-14 23:38:19,694 INFO] * number of parameters: 177854209\n",
      "[2022-05-14 23:38:19,883 INFO] * number of parameters: 177854209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n",
      "gpu_rank 0\n",
      "gpu_rank 0\n",
      "gpu_rank 0\n",
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-14 23:38:19,885 INFO] * number of parameters: 177854209\n",
      "[2022-05-14 23:38:19,996 INFO] * number of parameters: 177854209\n",
      "[2022-05-14 23:38:20,000 INFO] * number of parameters: 177854209\n",
      "[2022-05-14 23:38:20,002 INFO] * number of parameters: 177854209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n",
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-14 23:38:20,889 INFO] * number of parameters: 177854209\n",
      " 14%|█▍        | 2/14 [00:10<00:54,  4.55s/it][2022-05-14 23:38:21,758 INFO] * number of parameters: 177854209\n",
      "[2022-05-14 23:38:21,937 INFO] * number of parameters: 177854209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:11<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ThreadPool 방식\n",
    "pool = ThreadPool(50)\n",
    "\n",
    "new_list = []\n",
    "\n",
    "try:\n",
    "    for _ in tqdm(pool.imap_unordered(inference, idx_text), total=len(idx_text)):\n",
    "        pass\n",
    "    \n",
    "    print(\"finish\")\n",
    "    \n",
    "except RuntimeError:\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "    pool.join()\n",
    "    \n",
    "    df_covid.to_csv(\"../test_content_f_55.csv\")\n",
    "    \n",
    "    col_name = [\"index\", \"title\", \"content\", \"date\", \"url\", \"summary\"]\n",
    "    news_df = pd.DataFrame(new_list, columns=col_name)\n",
    "\n",
    "    csv_file_name = \"../test_summary_55.csv\" ### 이름 바꿔줘야함\n",
    "\n",
    "    news_df.to_csv(csv_file_name)\n",
    "    news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1ec7e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26764</td>\n",
       "      <td>“8번째 확진자 군산 대중목욕탕·대형마트 등 방문”</td>\n",
       "      <td>신종 코로나바이러스 감염증 국내 8번 확진자가 지난달 말 귀국한 뒤 대형마트와 목욕...</td>\n",
       "      <td>2020.2.2</td>\n",
       "      <td>https://www.hani.co.kr/arti/area/honam/926579....</td>\n",
       "      <td>전북도와 군산시 등은 2일 “8번 확진자인 62살 여성은 지난달 23일 중국 우한에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26854</td>\n",
       "      <td>[단독] 우한 알리는 중국 활동가의 페북 일기 “봉쇄가 공황 불러와”</td>\n",
       "      <td>신종 코로나 발병 이후 시민들의 이동이 통제되며 문을 닫는 우한의 상점들이 늘고 있...</td>\n",
       "      <td>2020.1.30</td>\n",
       "      <td>https://www.hani.co.kr/arti/society/society_ge...</td>\n",
       "      <td>생필품을 사기 위해 슈퍼마켓에 줄을 서 있는 우한 시민들. 쌀과 야채가 놓여져 있던...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26775</td>\n",
       "      <td>손끝까지 꼼꼼히, 마스크 철벽…‘셀프 방역’이 가장 확실한 방역</td>\n",
       "      <td>국내에서도 신종 코로나바이러스 감염증 ‘2차 전파’ 사례가 발생하면서 지역사회 감염...</td>\n",
       "      <td>2020.2.2</td>\n",
       "      <td>https://www.hani.co.kr/arti/economy/finance/92...</td>\n",
       "      <td>국내에서도 신종 코로나바이러스 감염증 ‘2차 전파’ 사례가 발생하면서 지역사회 감염...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26970</td>\n",
       "      <td>시중은행도 신종 코로나 비상대응 “손소독제 비치, 직원 마스크 착용”</td>\n",
       "      <td>케이이비(KEB)하나은행은 28일 오전 비상대책위원회를 열고 위기대응 단계를 ‘경계...</td>\n",
       "      <td>2020.1.28</td>\n",
       "      <td>https://www.hani.co.kr/arti/economy/finance/92...</td>\n",
       "      <td>케이이비(KEB)하나은행은 28일 오전 비상대책위원회를 열고 위기대응 단계를 ‘경계...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27218</td>\n",
       "      <td>새내기 민나온, 데뷔전에서 ‘톱10’</td>\n",
       "      <td>새내기 민나온(19)이 어렵게 찾아온 미국여자프로골프(LPGA) 투어 데뷔 무대에서...</td>\n",
       "      <td>2007.4.30</td>\n",
       "      <td>https://www.hani.co.kr/arti/sports/golf/206180...</td>\n",
       "      <td>새내기 민나온(19)이 어렵게 찾아온 미국여자프로골프(LPGA) 투어 데뷔 무대에서...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                   title  \\\n",
       "0  26764            “8번째 확진자 군산 대중목욕탕·대형마트 등 방문”   \n",
       "1  26854  [단독] 우한 알리는 중국 활동가의 페북 일기 “봉쇄가 공황 불러와”   \n",
       "2  26775     손끝까지 꼼꼼히, 마스크 철벽…‘셀프 방역’이 가장 확실한 방역   \n",
       "3  26970  시중은행도 신종 코로나 비상대응 “손소독제 비치, 직원 마스크 착용”   \n",
       "4  27218                    새내기 민나온, 데뷔전에서 ‘톱10’   \n",
       "\n",
       "                                             content       date  \\\n",
       "0  신종 코로나바이러스 감염증 국내 8번 확진자가 지난달 말 귀국한 뒤 대형마트와 목욕...   2020.2.2   \n",
       "1  신종 코로나 발병 이후 시민들의 이동이 통제되며 문을 닫는 우한의 상점들이 늘고 있...  2020.1.30   \n",
       "2  국내에서도 신종 코로나바이러스 감염증 ‘2차 전파’ 사례가 발생하면서 지역사회 감염...   2020.2.2   \n",
       "3  케이이비(KEB)하나은행은 28일 오전 비상대책위원회를 열고 위기대응 단계를 ‘경계...  2020.1.28   \n",
       "4  새내기 민나온(19)이 어렵게 찾아온 미국여자프로골프(LPGA) 투어 데뷔 무대에서...  2007.4.30   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.hani.co.kr/arti/area/honam/926579....   \n",
       "1  https://www.hani.co.kr/arti/society/society_ge...   \n",
       "2  https://www.hani.co.kr/arti/economy/finance/92...   \n",
       "3  https://www.hani.co.kr/arti/economy/finance/92...   \n",
       "4  https://www.hani.co.kr/arti/sports/golf/206180...   \n",
       "\n",
       "                                             summary  \n",
       "0  전북도와 군산시 등은 2일 “8번 확진자인 62살 여성은 지난달 23일 중국 우한에...  \n",
       "1  생필품을 사기 위해 슈퍼마켓에 줄을 서 있는 우한 시민들. 쌀과 야채가 놓여져 있던...  \n",
       "2  국내에서도 신종 코로나바이러스 감염증 ‘2차 전파’ 사례가 발생하면서 지역사회 감염...  \n",
       "3  케이이비(KEB)하나은행은 28일 오전 비상대책위원회를 열고 위기대응 단계를 ‘경계...  \n",
       "4  새내기 민나온(19)이 어렵게 찾아온 미국여자프로골프(LPGA) 투어 데뷔 무대에서...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위 코드 정상적으로 끝날 경우, 저장\n",
    "\n",
    "pool.close()\n",
    "pool.terminate()\n",
    "pool.join()\n",
    "    \n",
    "df_covid.to_csv(\"../test_content_f_55.csv\")\n",
    "    \n",
    "col_name = [\"index\", \"title\", \"content\", \"date\", \"url\", \"summary\"]\n",
    "news_df = pd.DataFrame(new_list, columns=col_name)\n",
    "\n",
    "csv_file_name = \"../test_summary_55.csv\" # 이름 바꿔줘야함\n",
    "\n",
    "news_df.to_csv(csv_file_name)\n",
    "news_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Notebook)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
