{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"textsummariation.ipynb","provenance":[],"collapsed_sections":["Y1KNhsxx0pep"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ed76b51cca234643ac2710f9e0fe88c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f244752eead4dbbabfa4467a9e83060","IPY_MODEL_4bdc89d4ab7646698dc999cbf34a721f","IPY_MODEL_e640ced8c8014a6c849d1526c90047c3"],"layout":"IPY_MODEL_f1fbd0603b864455b32a954777911a8a"}},"5f244752eead4dbbabfa4467a9e83060":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_043d72bc2f6a4745bb83e10f5d8cfcbc","placeholder":"​","style":"IPY_MODEL_4768327c3d35478c97944fc306e13fdf","value":"Downloading: 100%"}},"4bdc89d4ab7646698dc999cbf34a721f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85779815044d456fbe586778e0088386","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aff06f59ff5e4d8c9108bb65805e5d9c","value":625}},"e640ced8c8014a6c849d1526c90047c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bccc4124932b4dc7ad9df5012c4da73d","placeholder":"​","style":"IPY_MODEL_0af1f59aff3e416594910ece6d1d01c3","value":" 625/625 [00:00&lt;00:00, 27.7kB/s]"}},"f1fbd0603b864455b32a954777911a8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"043d72bc2f6a4745bb83e10f5d8cfcbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4768327c3d35478c97944fc306e13fdf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85779815044d456fbe586778e0088386":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aff06f59ff5e4d8c9108bb65805e5d9c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bccc4124932b4dc7ad9df5012c4da73d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0af1f59aff3e416594910ece6d1d01c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f09c37a126ca480b82de488a37c161ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f0c4625ae8c94476a630f0a62585799d","IPY_MODEL_42c9373a0b784394b9cb728db0da891e","IPY_MODEL_b0288e3c5cda406e99c912ea1bdf2f0d"],"layout":"IPY_MODEL_668f3877a18e411895737f38e4be5aea"}},"f0c4625ae8c94476a630f0a62585799d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b0b9564345d4e6fa97f358e54bf48fb","placeholder":"​","style":"IPY_MODEL_cdd2d0c6d6604188bd38d422fae3c816","value":"Downloading: 100%"}},"42c9373a0b784394b9cb728db0da891e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_436df110d99440feb32185c45c953aeb","max":672271273,"min":0,"orientation":"horizontal","style":"IPY_MODEL_832a38fee90946f998b641f939f97a45","value":672271273}},"b0288e3c5cda406e99c912ea1bdf2f0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bed98383bce84622b97e9355b8366da7","placeholder":"​","style":"IPY_MODEL_352e85981f0d4457b3e74abbeeaee43d","value":" 641M/641M [00:12&lt;00:00, 41.3MB/s]"}},"668f3877a18e411895737f38e4be5aea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b0b9564345d4e6fa97f358e54bf48fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdd2d0c6d6604188bd38d422fae3c816":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"436df110d99440feb32185c45c953aeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"832a38fee90946f998b641f939f97a45":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bed98383bce84622b97e9355b8366da7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"352e85981f0d4457b3e74abbeeaee43d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d396c0e0eb994132b023e8ea04f62296":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9346f52557940219f22db32c6bcda4d","IPY_MODEL_56ede677c97745bb8248918496f3f988","IPY_MODEL_323a1809bb6d4eeab6179dea43acbb32"],"layout":"IPY_MODEL_3f93cd504d114ecb8908757443e8fb08"}},"a9346f52557940219f22db32c6bcda4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf6b40b2e46746768f3164eae12276a6","placeholder":"​","style":"IPY_MODEL_07f8bf8e27a3469a93009ec08f808b18","value":"Downloading: 100%"}},"56ede677c97745bb8248918496f3f988":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9846cb3745a34183be148c86350650fe","max":871891,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13d2b37b076a4bbcad6056027ef72492","value":871891}},"323a1809bb6d4eeab6179dea43acbb32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10887d05b42c49168a99e23ee3a332ce","placeholder":"​","style":"IPY_MODEL_0d93b678ee7b41a2bb2db919322a8137","value":" 851k/851k [00:01&lt;00:00, 1.25MB/s]"}},"3f93cd504d114ecb8908757443e8fb08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf6b40b2e46746768f3164eae12276a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07f8bf8e27a3469a93009ec08f808b18":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9846cb3745a34183be148c86350650fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13d2b37b076a4bbcad6056027ef72492":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10887d05b42c49168a99e23ee3a332ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d93b678ee7b41a2bb2db919322a8137":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c13c48de0ca4dce9d81e74cf0ba44c2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_286a9cf1da554578a0292ac09ceccc87","IPY_MODEL_4b7755e86e7642569f00d82f4453f5c2","IPY_MODEL_f5e3c862a219460a8c324b7b03a5949e"],"layout":"IPY_MODEL_916e6070ef19471cbe1c92e796b04d85"}},"286a9cf1da554578a0292ac09ceccc87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81d148bb32a3499d8bc9e9853e8d9efb","placeholder":"​","style":"IPY_MODEL_e808c8acdeac46cd9010e4c027a1b17a","value":"Downloading: 100%"}},"4b7755e86e7642569f00d82f4453f5c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_802a6f0b723c49468be1d505ec7e6167","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_202a5c1d0c19474cafd080cf16af1b25","value":28}},"f5e3c862a219460a8c324b7b03a5949e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd247f51bf5f47619850bd820650d4fc","placeholder":"​","style":"IPY_MODEL_23a61055381840848ccdb6d9df4c6140","value":" 28.0/28.0 [00:00&lt;00:00, 158B/s]"}},"916e6070ef19471cbe1c92e796b04d85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81d148bb32a3499d8bc9e9853e8d9efb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e808c8acdeac46cd9010e4c027a1b17a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"802a6f0b723c49468be1d505ec7e6167":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"202a5c1d0c19474cafd080cf16af1b25":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd247f51bf5f47619850bd820650d4fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23a61055381840848ccdb6d9df4c6140":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3842cbe3dac84efdb04facc090e944f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0dfa998332e3419cb4de80146149eb2b","IPY_MODEL_a89c6f7ab1a54ae5aac451197a4fda74","IPY_MODEL_a4d3d9f3382d43c383628c90b9bb5716"],"layout":"IPY_MODEL_4f0ad73cbfc64a8cbe9482fe1351bb5a"}},"0dfa998332e3419cb4de80146149eb2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63af4e448a9943988b8d4c243ecfbdc9","placeholder":"​","style":"IPY_MODEL_2aa24e49047b4a9f9d95c0a0e62f2d18","value":"100%"}},"a89c6f7ab1a54ae5aac451197a4fda74":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd1a9523baa74c349a3a026037d61a66","max":14669,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f10647888db54d5590a247fcb2f22060","value":14669}},"a4d3d9f3382d43c383628c90b9bb5716":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34944f3d5d6646fc9348c8b74bb26012","placeholder":"​","style":"IPY_MODEL_8cbf6b8ee035461093c885045617cbf4","value":" 14669/14669 [23:55&lt;00:00,  9.19it/s]"}},"4f0ad73cbfc64a8cbe9482fe1351bb5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63af4e448a9943988b8d4c243ecfbdc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aa24e49047b4a9f9d95c0a0e62f2d18":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd1a9523baa74c349a3a026037d61a66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f10647888db54d5590a247fcb2f22060":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"34944f3d5d6646fc9348c8b74bb26012":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cbf6b8ee035461093c885045617cbf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7453af9fc72747e5921b8d48fce97618":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f053b33b598b424396907bceea40ec8b","IPY_MODEL_877a9c5ca0964bedb7d826c6d85c31e3","IPY_MODEL_d3b837359d594acb80360af4c32ecde8"],"layout":"IPY_MODEL_0c3c7cf67f074e18aa65e8d372f4c8e5"}},"f053b33b598b424396907bceea40ec8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33b57cf98b9c4a998f9d0ed48500f8ae","placeholder":"​","style":"IPY_MODEL_c96e65ac2b214973bcbc842aed364ce6","value":"100%"}},"877a9c5ca0964bedb7d826c6d85c31e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aff16c9505cf4f48880064fed6354f07","max":3766,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5de80ef10eb84886af2d8a8d598927e8","value":3766}},"d3b837359d594acb80360af4c32ecde8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_480728a259884233ab4474b3e101ae30","placeholder":"​","style":"IPY_MODEL_28029d4f0d13447d8e8d00deee1f618c","value":" 3766/3766 [10:32&lt;00:00,  5.83it/s]"}},"0c3c7cf67f074e18aa65e8d372f4c8e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33b57cf98b9c4a998f9d0ed48500f8ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c96e65ac2b214973bcbc842aed364ce6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aff16c9505cf4f48880064fed6354f07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5de80ef10eb84886af2d8a8d598927e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"480728a259884233ab4474b3e101ae30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28029d4f0d13447d8e8d00deee1f618c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"605079840ad44c3ba11a2faa397a40f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7918737b7e346639b9a25af9158f9a2","IPY_MODEL_f621633f1ac64720b67af887628f204c","IPY_MODEL_650ee17ca1c34240b59478d1f43259d7"],"layout":"IPY_MODEL_961a4456d10d42b5a31c8f47ed70438d"}},"a7918737b7e346639b9a25af9158f9a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fbb33802ba340df91ac0b69e8a5785b","placeholder":"​","style":"IPY_MODEL_43d58aec28c94e989c13a64026a1055e","value":"Downloading: 100%"}},"f621633f1ac64720b67af887628f204c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_911355399ddd481eb476b00b658a18aa","max":4123,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f495c3c75ee34df391382ef5092062f2","value":4123}},"650ee17ca1c34240b59478d1f43259d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bab3a380c5af4b7ab598e239d036ac64","placeholder":"​","style":"IPY_MODEL_c897eaab263149f3be6ee1ef6d68d2d8","value":" 4.12k/4.12k [00:00&lt;00:00, 149kB/s]"}},"961a4456d10d42b5a31c8f47ed70438d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fbb33802ba340df91ac0b69e8a5785b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43d58aec28c94e989c13a64026a1055e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"911355399ddd481eb476b00b658a18aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f495c3c75ee34df391382ef5092062f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bab3a380c5af4b7ab598e239d036ac64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c897eaab263149f3be6ee1ef6d68d2d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/gist/youlive789/7b704c29d6d144bdc837eda85aff7682/textsummariation-3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"0q9fa0RSy2XL"},"source":["## 라이브러리 설치"]},{"cell_type":"code","metadata":{"id":"3KNjacmDyqcN"},"source":["!pip install git+https://github.com/huggingface/transformers\n","!pip list | grep -E 'transformers|tokenizers'\n","\n","# !git clone https://github.com/huggingface/nlp.git\n","!pip install git+git://github.com/huggingface/nlp.git@0.4.0\n","!pip install --force-reinstall pyarrow==0.16.0\n","\n","!pip install rouge_score rouge_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w5KwOko00LG0"},"source":["## 구글 드라이브 마운트"]},{"cell_type":"code","metadata":{"id":"06qIhsiQy44Z"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y1KNhsxx0pep"},"source":["## 데이터 준비\n","준비한 데이터를 판다스로 불러오고 huggingface의 nlp 라이브러리를 이용하여 학습 데이터를 만들어줍니다.\n","  \n","  \n","**주의점**\n"," - 이 노트에서 사용한 데이터는 매우 작습니다. 실제 테스트 하실 때에는 충분히 데이터를 확보해주세요."]},{"cell_type":"code","metadata":{"id":"HkOg7E8Q0vsu","colab":{"base_uri":"https://localhost:8080/","height":337},"outputId":"02041570-4d78-4244-e461-ffef6aacb38d","executionInfo":{"status":"ok","timestamp":1650607276857,"user_tz":-540,"elapsed":15566,"user":{"displayName":"‍조나영(학부생-소프트웨어전공)","userId":"10965658488015954600"}}},"source":["import pandas as pd\n","\n","# input raw data\n","raw_data_train = pd.read_csv(\"/content/gdrive/MyDrive/Colab_Notebooks/capstone-29/news-topic/summarization_data/json_to_csv_train_신문기사.csv\").drop(['Unnamed: 0'],axis=1)\n","raw_data_valid = pd.read_csv(\"/content/gdrive/MyDrive/Colab_Notebooks/capstone-29/news-topic/summarization_data/json_to_csv_valid_신문기사.csv\").drop(['Unnamed: 0'],axis=1)\n","raw_data_valid.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  media         id                                    article_orginal  \\\n","0  한국경제  340626877  ['[ 박재원 기자 ] \\'대한민국 5G 홍보대사\\'를 자처한 문재인 대통령은 \"넓...   \n","1  한국경제  340626896  ['] 당 지도부 퇴진을 놓고 바른미래당 내홍이 격화되고 있다.', '바른미래당이 ...   \n","2  한국경제  340626904  ['[ 홍윤정 기자 ] 8일 서울 올림픽공원 K아트홀.', \"지난 3일 한국이 세계...   \n","3  한국경제  340627450  ['] 박원순 서울시장(사진)이 8일 고층 재개발·재건축 관련 요구에 작심한 듯 쓴...   \n","4  한국경제  340627465  ['[ 임근호 기자 ] \"SK(주)와 미국 알파벳(구글 지주회사)의 간결한 지배구조...   \n","\n","                                         abstractive extractive  \n","0  8일 서울에서 열린 5G플러스 전략발표에 참석한 문재인 대통령은 5G는 대한민국 혁...  [0, 1, 3]  \n","1  8일 바른미래당 최고의원 회의에 하태경 의원 등 5명의 최고의원이 지도부 퇴진을 요...  [2, 1, 6]  \n","2  지난 3일 한국이 세계 첫 5세대 이동통신 서비스를 보편화한 것을 축하하는 '코리안...  [1, 5, 8]  \n","3  박원순 서울시장은 8일 서울시청에서 열린 '골목길 재생 시민 정책 대화'에 참석하여...  [0, 1, 2]  \n","4  주주가치 포커스를 운용하는 KB자산운용이  SK와 알파벳(구글 지주회사)의 모범적 ...  [1, 3, 4]  "],"text/html":["\n","  <div id=\"df-ee9167be-48b2-4256-aaee-3a6abb0db89d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>media</th>\n","      <th>id</th>\n","      <th>article_orginal</th>\n","      <th>abstractive</th>\n","      <th>extractive</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>한국경제</td>\n","      <td>340626877</td>\n","      <td>['[ 박재원 기자 ] \\'대한민국 5G 홍보대사\\'를 자처한 문재인 대통령은 \"넓...</td>\n","      <td>8일 서울에서 열린 5G플러스 전략발표에 참석한 문재인 대통령은 5G는 대한민국 혁...</td>\n","      <td>[0, 1, 3]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>한국경제</td>\n","      <td>340626896</td>\n","      <td>['] 당 지도부 퇴진을 놓고 바른미래당 내홍이 격화되고 있다.', '바른미래당이 ...</td>\n","      <td>8일 바른미래당 최고의원 회의에 하태경 의원 등 5명의 최고의원이 지도부 퇴진을 요...</td>\n","      <td>[2, 1, 6]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>한국경제</td>\n","      <td>340626904</td>\n","      <td>['[ 홍윤정 기자 ] 8일 서울 올림픽공원 K아트홀.', \"지난 3일 한국이 세계...</td>\n","      <td>지난 3일 한국이 세계 첫 5세대 이동통신 서비스를 보편화한 것을 축하하는 '코리안...</td>\n","      <td>[1, 5, 8]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>한국경제</td>\n","      <td>340627450</td>\n","      <td>['] 박원순 서울시장(사진)이 8일 고층 재개발·재건축 관련 요구에 작심한 듯 쓴...</td>\n","      <td>박원순 서울시장은 8일 서울시청에서 열린 '골목길 재생 시민 정책 대화'에 참석하여...</td>\n","      <td>[0, 1, 2]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>한국경제</td>\n","      <td>340627465</td>\n","      <td>['[ 임근호 기자 ] \"SK(주)와 미국 알파벳(구글 지주회사)의 간결한 지배구조...</td>\n","      <td>주주가치 포커스를 운용하는 KB자산운용이  SK와 알파벳(구글 지주회사)의 모범적 ...</td>\n","      <td>[1, 3, 4]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee9167be-48b2-4256-aaee-3a6abb0db89d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ee9167be-48b2-4256-aaee-3a6abb0db89d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ee9167be-48b2-4256-aaee-3a6abb0db89d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import pandas as pd\n","\n","df_train = [\n","]\n","col = ['short', 'long']\n","df_train = pd.DataFrame(df_train, columns=col)\n","df_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"id":"k1zcmmJw2ZZO","executionInfo":{"status":"ok","timestamp":1650607442481,"user_tz":-540,"elapsed":306,"user":{"displayName":"‍조나영(학부생-소프트웨어전공)","userId":"10965658488015954600"}},"outputId":"e94e0e34-a7ea-42b6-e40a-6f43874ce7a6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [short, long]\n","Index: []"],"text/html":["\n","  <div id=\"df-1081919e-f935-4b9f-aae6-1e741e324fe3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>short</th>\n","      <th>long</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1081919e-f935-4b9f-aae6-1e741e324fe3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1081919e-f935-4b9f-aae6-1e741e324fe3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1081919e-f935-4b9f-aae6-1e741e324fe3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","for i in tqdm(range(len(raw_data_train))):\n","  data = raw_data_train.loc[i]\n","  short_data = data[3]\n","  long_data = data[2][2:-2]\n","\n","  table = long_data.maketrans({ ',': '', \"'\": ''})\n","  long_data = long_data.translate(table)\n","\n","  new_data = {'short' : short_data, 'long' : long_data}\n","  df_train = df_train.append(new_data, ignore_index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3mplGaFM3tkU","executionInfo":{"status":"ok","timestamp":1650611116004,"user_tz":-540,"elapsed":2907490,"user":{"displayName":"‍조나영(학부생-소프트웨어전공)","userId":"10965658488015954600"}},"outputId":"7526e524-15c0-4b18-c298-527b4d5321d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 243983/243983 [48:27<00:00, 83.92it/s]\n"]}]},{"cell_type":"code","source":["df_train.to_csv('/content/gdrive/MyDrive/Colab_Notebooks/capstone-29/news-topic/summarization_data/csv_train.csv', encoding='utf-8-sig')"],"metadata":{"id":"63Xe_QjK67sJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","df_valid = [\n","]\n","col = ['short', 'long']\n","df_valid = pd.DataFrame(df_valid, columns=col)\n","df_valid"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"id":"Tq5xRfrG6pdH","executionInfo":{"status":"ok","timestamp":1650611205424,"user_tz":-540,"elapsed":329,"user":{"displayName":"‍조나영(학부생-소프트웨어전공)","userId":"10965658488015954600"}},"outputId":"4d645276-9850-45ed-f54a-5170112990ca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [short, long]\n","Index: []"],"text/html":["\n","  <div id=\"df-4afa31dc-bf45-4ac4-8e3d-bc651230cad2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>short</th>\n","      <th>long</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4afa31dc-bf45-4ac4-8e3d-bc651230cad2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4afa31dc-bf45-4ac4-8e3d-bc651230cad2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4afa31dc-bf45-4ac4-8e3d-bc651230cad2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","for i in tqdm(range(len(raw_data_valid))):\n","  data = raw_data_valid.loc[i]\n","  short_data = data[3]\n","  long_data = data[2][2:-2]\n","\n","  table = long_data.maketrans({ ',': '', \"'\": ''})\n","  long_data = long_data.translate(table)\n","\n","  new_data = {'short' : short_data, 'long' : long_data}\n","  df_valid = df_valid.append(new_data, ignore_index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0-DFoKz6uBy","executionInfo":{"status":"ok","timestamp":1650612958381,"user_tz":-540,"elapsed":176232,"user":{"displayName":"‍조나영(학부생-소프트웨어전공)","userId":"10965658488015954600"}},"outputId":"4f4a4800-b015-4f0f-e565-85dc5117e13d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30122/30122 [02:56<00:00, 170.99it/s]\n"]}]},{"cell_type":"code","source":["df_valid.to_csv('/content/gdrive/MyDrive/Colab_Notebooks/capstone-29/news-topic/summarization_data/csv_valid.csv', encoding='utf-8-sig')"],"metadata":{"id":"YRPWl6dC7ES4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 데이터 세팅"],"metadata":{"id":"G3gCToTb4s7A"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df_train = pd.read_csv(\"/content/gdrive/MyDrive/csv_train.csv\").drop(['Unnamed: 0'],axis=1)\n","df_valid = pd.read_csv(\"/content/gdrive/MyDrive/csv_valid.csv\").drop(['Unnamed: 0'],axis=1)"],"metadata":{"id":"7Zlw4CVYoBMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(df_train))\n","print(len(df_valid))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZXIbaRAM0OP","executionInfo":{"status":"ok","timestamp":1651149170927,"user_tz":-540,"elapsed":8,"user":{"displayName":"조나영","userId":"08548129972931830779"}},"outputId":"4a5aadbe-479d-4297-a27c-1ffd4f64b783"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["243983\n","60244\n"]}]},{"cell_type":"code","source":["df_train_dn = df_train.dropna(how='any')\n","df_valid_dn = df_valid.dropna(how='any')\n","\n","print(len(df_train_dn))\n","print(len(df_valid_dn))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXT4ERpKL7Jf","executionInfo":{"status":"ok","timestamp":1651149170927,"user_tz":-540,"elapsed":6,"user":{"displayName":"조나영","userId":"08548129972931830779"}},"outputId":"ff2bfeb8-79f6-4901-9054-7f10dcc8309b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["234702\n","60244\n"]}]},{"cell_type":"code","source":["pip install nlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_ci9M5oG5eL","executionInfo":{"status":"ok","timestamp":1651149176243,"user_tz":-540,"elapsed":5319,"user":{"displayName":"조나영","userId":"08548129972931830779"}},"outputId":"8706ed66-3414-435f-a08c-5074b7c7487f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nlp\n","  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n","\u001b[?25l\r\u001b[K     |▏                               | 10 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 194 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 215 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 225 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 235 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 245 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 256 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 266 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 276 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 286 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 296 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 307 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 317 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 327 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 337 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 348 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 358 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 368 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 378 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 389 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 399 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 409 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 419 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 430 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 440 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 450 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 460 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 471 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 481 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 491 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 501 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 512 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 522 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 532 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 542 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 552 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 563 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 573 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 583 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 593 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 604 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 614 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 624 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 634 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 645 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 655 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 665 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 675 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 686 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 696 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 706 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 716 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 727 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 737 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 747 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 757 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 768 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 778 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 788 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 798 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 808 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 819 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 829 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 839 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 849 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 860 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 870 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 880 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 890 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 901 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 911 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 921 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 931 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 942 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 952 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 962 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 972 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 983 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 993 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.5 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.6 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlp) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from nlp) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from nlp) (3.6.0)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 20.9 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from nlp) (0.3.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nlp) (1.3.5)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (0.16.0)\n","Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pyarrow>=0.16.0->nlp) (1.16.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2.10)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2022.1)\n","Installing collected packages: xxhash, nlp\n","Successfully installed nlp-0.4.0 xxhash-3.0.0\n"]}]},{"cell_type":"code","metadata":{"id":"ZM60IiZz0RPc"},"source":["import nlp\n","train_dataset = nlp.Dataset.from_pandas(df_train_dn)\n","val_dataset = nlp.Dataset.from_pandas(df_valid_dn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1dtlcInI1XSM"},"source":["## 학습데이터 전처리 및 모델정의"]},{"cell_type":"code","metadata":{"id":"OR59Fptw1YKp"},"source":[" import torch\n"," from transformers import EncoderDecoderModel, BertTokenizer, Trainer, TrainingArguments"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ugbNf6oU158c"},"source":["Bert multilingual 모델을 encoder, decoder로 연결해줍니다. 또한 bert multilingual용 토크나이저도 생성해줍니다."]},{"cell_type":"code","metadata":{"id":"MijCIG5z1bsM","colab":{"base_uri":"https://localhost:8080/","height":301,"referenced_widgets":["ed76b51cca234643ac2710f9e0fe88c4","5f244752eead4dbbabfa4467a9e83060","4bdc89d4ab7646698dc999cbf34a721f","e640ced8c8014a6c849d1526c90047c3","f1fbd0603b864455b32a954777911a8a","043d72bc2f6a4745bb83e10f5d8cfcbc","4768327c3d35478c97944fc306e13fdf","85779815044d456fbe586778e0088386","aff06f59ff5e4d8c9108bb65805e5d9c","bccc4124932b4dc7ad9df5012c4da73d","0af1f59aff3e416594910ece6d1d01c3","f09c37a126ca480b82de488a37c161ea","f0c4625ae8c94476a630f0a62585799d","42c9373a0b784394b9cb728db0da891e","b0288e3c5cda406e99c912ea1bdf2f0d","668f3877a18e411895737f38e4be5aea","6b0b9564345d4e6fa97f358e54bf48fb","cdd2d0c6d6604188bd38d422fae3c816","436df110d99440feb32185c45c953aeb","832a38fee90946f998b641f939f97a45","bed98383bce84622b97e9355b8366da7","352e85981f0d4457b3e74abbeeaee43d","d396c0e0eb994132b023e8ea04f62296","a9346f52557940219f22db32c6bcda4d","56ede677c97745bb8248918496f3f988","323a1809bb6d4eeab6179dea43acbb32","3f93cd504d114ecb8908757443e8fb08","cf6b40b2e46746768f3164eae12276a6","07f8bf8e27a3469a93009ec08f808b18","9846cb3745a34183be148c86350650fe","13d2b37b076a4bbcad6056027ef72492","10887d05b42c49168a99e23ee3a332ce","0d93b678ee7b41a2bb2db919322a8137","2c13c48de0ca4dce9d81e74cf0ba44c2","286a9cf1da554578a0292ac09ceccc87","4b7755e86e7642569f00d82f4453f5c2","f5e3c862a219460a8c324b7b03a5949e","916e6070ef19471cbe1c92e796b04d85","81d148bb32a3499d8bc9e9853e8d9efb","e808c8acdeac46cd9010e4c027a1b17a","802a6f0b723c49468be1d505ec7e6167","202a5c1d0c19474cafd080cf16af1b25","cd247f51bf5f47619850bd820650d4fc","23a61055381840848ccdb6d9df4c6140"]},"outputId":"f2cc8b6d-7f08-408a-ddda-7da08f3823c8","executionInfo":{"status":"ok","timestamp":1651149213313,"user_tz":-540,"elapsed":28601,"user":{"displayName":"조나영","userId":"08548129972931830779"}}},"source":["model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-multilingual-uncased', 'bert-base-multilingual-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed76b51cca234643ac2710f9e0fe88c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/641M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f09c37a126ca480b82de488a37c161ea"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/851k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d396c0e0eb994132b023e8ea04f62296"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c13c48de0ca4dce9d81e74cf0ba44c2"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"-8glIBqE2Q_0"},"source":["map_to_encoder_decoder_inputs : 문장을 bert에 입력할 수 있도록 전처리를 수행합니다.  \n","compute_metrics : rouge2 점수를 통해 성능을 측정합니다.\n","\n","tokenizer 안의 max_length를 통해 input과 output의 길이를 조정할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"ADam1-CV1iTL"},"source":["from sklearn.metrics import accuracy_score\n","\n","def map_to_encoder_decoder_inputs(batch):\n","    inputs = tokenizer(batch[\"long\"], padding=\"max_length\", truncation=True, max_length=512) # 요약하고자 하는 문장\n","    outputs = tokenizer(batch[\"short\"], padding=\"max_length\", truncation=True, max_length=512) # 요약이 된 정답 문장\n","\n","    batch[\"input_ids\"] = inputs.input_ids\n","    batch[\"attention_mask\"] = inputs.attention_mask\n","\n","    batch[\"decoder_input_ids\"] = outputs.input_ids\n","    batch[\"labels\"] = outputs.input_ids.copy()\n","\n","    batch[\"labels\"] = [\n","        [-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]\n","    ]\n","    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n","\n","    assert all([len(x) == 512 for x in inputs.input_ids])\n","    assert all([len(x) == 512 for x in outputs.input_ids])\n","\n","    return batch\n","\n","def compute_metrics(pred):\n","  labels_ids = pred.label_ids\n","  pred_ids = pred.predictions\n","  preds = pred.predictions.argmax(-1)\n","\n","  pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","  label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","  rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n","\n","  acc = accuracy_score(labels_ids, preds)\n","  # print(round(rouge_output.recall, 4))\n","\n","  return {\n","      \"accuray\": acc,\n","      \"rouge2_precision\": round(rouge_output.precision, 4),\n","      \"rouge2_recall\": round(rouge_output.recall, 4),\n","      \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"klOFEjZt2jxh"},"source":["학습데이터와 검증데이터를 나누어 전처리를 수행합니다."]},{"cell_type":"code","metadata":{"id":"xz3E4NaW1nme","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["3842cbe3dac84efdb04facc090e944f3","0dfa998332e3419cb4de80146149eb2b","a89c6f7ab1a54ae5aac451197a4fda74","a4d3d9f3382d43c383628c90b9bb5716","4f0ad73cbfc64a8cbe9482fe1351bb5a","63af4e448a9943988b8d4c243ecfbdc9","2aa24e49047b4a9f9d95c0a0e62f2d18","fd1a9523baa74c349a3a026037d61a66","f10647888db54d5590a247fcb2f22060","34944f3d5d6646fc9348c8b74bb26012","8cbf6b8ee035461093c885045617cbf4"]},"outputId":"f644a859-c9cd-4801-96dc-d55b84181dd1","executionInfo":{"status":"ok","timestamp":1651150648429,"user_tz":-540,"elapsed":1435120,"user":{"displayName":"조나영","userId":"08548129972931830779"}}},"source":["batch_size_train = 32\n","\n","train_dataset = train_dataset.map(\n","    map_to_encoder_decoder_inputs, batched=True, batch_size=batch_size_train, remove_columns=[\"short\", \"long\"],\n",")\n","train_dataset.set_format(\n","    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/14669 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3842cbe3dac84efdb04facc090e944f3"}},"metadata":{}}]},{"cell_type":"code","source":["batch_size_valid = 64\n","\n","val_dataset = val_dataset.map(\n","    map_to_encoder_decoder_inputs, batched=True, batch_size=batch_size_valid, remove_columns=[\"short\", \"long\"],\n",")\n","val_dataset.set_format(\n","    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",")"],"metadata":{"id":"J2-r8b8dkADE","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["7453af9fc72747e5921b8d48fce97618","f053b33b598b424396907bceea40ec8b","877a9c5ca0964bedb7d826c6d85c31e3","d3b837359d594acb80360af4c32ecde8","0c3c7cf67f074e18aa65e8d372f4c8e5","33b57cf98b9c4a998f9d0ed48500f8ae","c96e65ac2b214973bcbc842aed364ce6","aff16c9505cf4f48880064fed6354f07","5de80ef10eb84886af2d8a8d598927e8","480728a259884233ab4474b3e101ae30","28029d4f0d13447d8e8d00deee1f618c"]},"executionInfo":{"status":"ok","timestamp":1651151280531,"user_tz":-540,"elapsed":632113,"user":{"displayName":"조나영","userId":"08548129972931830779"}},"outputId":"376c98a5-db6d-4fe2-c2cb-4cb2d98ad9ff"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3766 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7453af9fc72747e5921b8d48fce97618"}},"metadata":{}}]},{"cell_type":"code","source":["# from transformers import AdamW\n","\n","# no_decay = ['bias', 'LayerNorm.weight']\n","# optimizer_grouped_parameters = [\n","#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","# ]\n","\n","# optimizer = AdamW(\n","#     optimizer_grouped_parameters,\n","#     lr=0.001,\n","#     eps=1e-06,\n","#     correct_bias=True,\n","#     no_deprecation_warning=True\n","#     )"],"metadata":{"id":"5D-8kXMqZry0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Trainer - loss function 추가\n","\n","class MyTrainer(Trainer):\n","      # loss_name 이라는 인자를 추가로 받아 self에 각인 시켜줍니다.\n","      def __init__(self, loss_name, *args, **kwargs):\n","          super().__init__(*args, **kwargs)\n","          self.loss_name= loss_name # 각인!\n","\n","      def compute_loss(self, model, inputs, return_outputs=False):\n","        # config에 저장된 loss_name에 따라 다른 loss 계산 \n","        if self.loss_name == 'CrossEntropy':\n","            # lossname이 CrossEntropy 이면, custom_loss에 torch.nn.CrossEntropyLoss()를 선언(?) 해줍니다.\n","            custom_loss = torch.nn.CrossEntropyLoss()\n","                      \n","        if self.label_smoother is not None and \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None\n","\n","        outputs = model(**inputs)\n","\n","        if labels is not None:\n","            #loss를 계산 하던 부분에 custom_loss를 이용해 계산하는 코드를 넣어 줍니다!\n","            #원본 코드를 보시면 output[0]가 logit 임을 알 수 있습니다!\n","            loss = custom_loss(outputs[0], labels)\n","        else:\n","            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","        return (loss, outputs) if return_outputs else loss"],"metadata":{"id":"nnpCIovFg1rT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U2OQQUgV2vlh"},"source":["모델에 필요한 환경설정을 해줍니다."]},{"cell_type":"code","metadata":{"id":"PCnSnamE2y1I","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["605079840ad44c3ba11a2faa397a40f3","a7918737b7e346639b9a25af9158f9a2","f621633f1ac64720b67af887628f204c","650ee17ca1c34240b59478d1f43259d7","961a4456d10d42b5a31c8f47ed70438d","6fbb33802ba340df91ac0b69e8a5785b","43d58aec28c94e989c13a64026a1055e","911355399ddd481eb476b00b658a18aa","f495c3c75ee34df391382ef5092062f2","bab3a380c5af4b7ab598e239d036ac64","c897eaab263149f3be6ee1ef6d68d2d8"]},"outputId":"010d05ba-24e6-4680-f96b-ab84798b6823","executionInfo":{"status":"ok","timestamp":1651151295450,"user_tz":-540,"elapsed":14923,"user":{"displayName":"조나영","userId":"08548129972931830779"}}},"source":["tokenizer.bos_token = tokenizer.cls_token\n","tokenizer.eos_token = tokenizer.sep_token\n","rouge = nlp.load_metric(\"rouge\")\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n","\n","model.config.decoder_start_token_id = tokenizer.bos_token_id\n","model.config.eos_token_id = tokenizer.eos_token_id\n","model.config.max_length = 256\n","model.config.min_length = 512\n","model.config.no_repeat_ngram_size = 3\n","model.early_stopping = True\n","model.length_penalty = 2.0\n","model.num_beams = 4"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/4.12k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"605079840ad44c3ba11a2faa397a40f3"}},"metadata":{}}]},{"cell_type":"code","source":["print(device)"],"metadata":{"id":"CbpNAzFqKeem"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PXUGdNj43D1x"},"source":["## 모델학습\n","\n","trainer를 통해 모델을 학습시킵니다."]},{"cell_type":"code","metadata":{"id":"TKWTovLt25Bh"},"source":["training_args = TrainingArguments(\n","    output_dir=\"/content/gdrive/MyDrive/results_2\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=batch_size_train,\n","    per_device_eval_batch_size=batch_size_valid,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    do_train=True,\n","    do_eval=True,\n","    logging_steps=30,\n","    save_steps=5000,\n","    eval_steps=500,\n","    overwrite_output_dir=True,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n",")\n","\n","# trainer = MyTrainer(\n","#       loss_name='CrossEntropy',\n","#       model=model,\n","#       args=training_args,\n","#       train_dataset=train_dataset,\n","#       eval_dataset=val_dataset,\n","#       compute_metrics=compute_metrics,\n","#       # optimizers=(optimizer, transformers.get_linear_schedule_with_warmup())\n","#       )"],"metadata":{"id":"gFWWsB_dfpmx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3a-yl-xHfr3C","outputId":"07b059c9-65ba-409b-c523-074c1ea512a5","executionInfo":{"status":"error","timestamp":1651160689729,"user_tz":-540,"elapsed":9394287,"user":{"displayName":"조나영","userId":"08548129972931830779"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 234702\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 44007\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='14748' max='44007' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [14748/44007 2:36:31 < 5:10:33, 1.57 it/s, Epoch 1.01/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>30</td>\n","      <td>2.794100</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.829400</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.151700</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.032800</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.017300</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.010700</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.006900</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.003100</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.002900</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.002800</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.004700</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.003700</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.002900</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.001800</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.003000</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.002900</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>0.000900</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>0.001800</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>0.002900</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.001600</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>0.001400</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>0.003000</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>0.001300</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.001000</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>0.001000</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>0.001700</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>0.000600</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>0.000400</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>1020</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1080</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1170</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>1230</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>1260</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>1290</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>1320</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>0.001700</td>\n","    </tr>\n","    <tr>\n","      <td>1380</td>\n","      <td>0.000500</td>\n","    </tr>\n","    <tr>\n","      <td>1410</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>1440</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>1470</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>1530</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>1560</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>1590</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>1620</td>\n","      <td>0.001100</td>\n","    </tr>\n","    <tr>\n","      <td>1650</td>\n","      <td>0.000900</td>\n","    </tr>\n","    <tr>\n","      <td>1680</td>\n","      <td>0.002200</td>\n","    </tr>\n","    <tr>\n","      <td>1710</td>\n","      <td>0.001600</td>\n","    </tr>\n","    <tr>\n","      <td>1740</td>\n","      <td>0.001100</td>\n","    </tr>\n","    <tr>\n","      <td>1770</td>\n","      <td>0.000900</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.000400</td>\n","    </tr>\n","    <tr>\n","      <td>1830</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>1860</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1890</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>1920</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1980</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2010</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2040</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2070</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2130</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2160</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2190</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.001100</td>\n","    </tr>\n","    <tr>\n","      <td>2310</td>\n","      <td>0.002200</td>\n","    </tr>\n","    <tr>\n","      <td>2340</td>\n","      <td>0.002100</td>\n","    </tr>\n","    <tr>\n","      <td>2370</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.002000</td>\n","    </tr>\n","    <tr>\n","      <td>2430</td>\n","      <td>0.000400</td>\n","    </tr>\n","    <tr>\n","      <td>2460</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>2490</td>\n","      <td>0.003700</td>\n","    </tr>\n","    <tr>\n","      <td>2520</td>\n","      <td>0.013300</td>\n","    </tr>\n","    <tr>\n","      <td>2550</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>2580</td>\n","      <td>0.006800</td>\n","    </tr>\n","    <tr>\n","      <td>2610</td>\n","      <td>0.002300</td>\n","    </tr>\n","    <tr>\n","      <td>2640</td>\n","      <td>0.000500</td>\n","    </tr>\n","    <tr>\n","      <td>2670</td>\n","      <td>0.000800</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>2730</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>2760</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2790</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>2820</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>2850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2880</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>2910</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2940</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2970</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>3030</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>3060</td>\n","      <td>0.000400</td>\n","    </tr>\n","    <tr>\n","      <td>3090</td>\n","      <td>0.000400</td>\n","    </tr>\n","    <tr>\n","      <td>3120</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>3150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3180</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>3210</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>3240</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3270</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>0.000800</td>\n","    </tr>\n","    <tr>\n","      <td>3360</td>\n","      <td>0.000600</td>\n","    </tr>\n","    <tr>\n","      <td>3390</td>\n","      <td>0.000800</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>3450</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>3480</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>3510</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>3540</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>3570</td>\n","      <td>0.000600</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>3630</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3660</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>3690</td>\n","      <td>0.000500</td>\n","    </tr>\n","    <tr>\n","      <td>3720</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>3750</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>3780</td>\n","      <td>0.000500</td>\n","    </tr>\n","    <tr>\n","      <td>3810</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>3840</td>\n","      <td>0.000800</td>\n","    </tr>\n","    <tr>\n","      <td>3870</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>3930</td>\n","      <td>0.001700</td>\n","    </tr>\n","    <tr>\n","      <td>3960</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>3990</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>4020</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4050</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>4080</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>4110</td>\n","      <td>0.000900</td>\n","    </tr>\n","    <tr>\n","      <td>4140</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>4170</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>4230</td>\n","      <td>0.000600</td>\n","    </tr>\n","    <tr>\n","      <td>4260</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>4290</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>4320</td>\n","      <td>0.000800</td>\n","    </tr>\n","    <tr>\n","      <td>4350</td>\n","      <td>0.006600</td>\n","    </tr>\n","    <tr>\n","      <td>4380</td>\n","      <td>0.000600</td>\n","    </tr>\n","    <tr>\n","      <td>4410</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>4470</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4530</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4560</td>\n","      <td>0.000600</td>\n","    </tr>\n","    <tr>\n","      <td>4590</td>\n","      <td>0.004600</td>\n","    </tr>\n","    <tr>\n","      <td>4620</td>\n","      <td>0.001000</td>\n","    </tr>\n","    <tr>\n","      <td>4650</td>\n","      <td>0.001500</td>\n","    </tr>\n","    <tr>\n","      <td>4680</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>4710</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4740</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4770</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4830</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4860</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4890</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4920</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4980</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5010</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5040</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5070</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5130</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5160</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5190</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5220</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5280</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5310</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5340</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5370</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5430</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5460</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5490</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5520</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5580</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5610</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5640</td>\n","      <td>0.000900</td>\n","    </tr>\n","    <tr>\n","      <td>5670</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000600</td>\n","    </tr>\n","    <tr>\n","      <td>5730</td>\n","      <td>0.001700</td>\n","    </tr>\n","    <tr>\n","      <td>5760</td>\n","      <td>0.001600</td>\n","    </tr>\n","    <tr>\n","      <td>5790</td>\n","      <td>0.061500</td>\n","    </tr>\n","    <tr>\n","      <td>5820</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>5850</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>5880</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>5910</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5940</td>\n","      <td>0.000800</td>\n","    </tr>\n","    <tr>\n","      <td>5970</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.003900</td>\n","    </tr>\n","    <tr>\n","      <td>6030</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>6060</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>6090</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6120</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6180</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6210</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6240</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6270</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6330</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6360</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6390</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6420</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6480</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6510</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6540</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6570</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6630</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6690</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6720</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6780</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6810</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6840</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6870</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6930</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6960</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6990</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7020</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7080</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7110</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7140</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7170</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7230</td>\n","      <td>0.001800</td>\n","    </tr>\n","    <tr>\n","      <td>7260</td>\n","      <td>0.003000</td>\n","    </tr>\n","    <tr>\n","      <td>7290</td>\n","      <td>0.001400</td>\n","    </tr>\n","    <tr>\n","      <td>7320</td>\n","      <td>0.002000</td>\n","    </tr>\n","    <tr>\n","      <td>7350</td>\n","      <td>0.001000</td>\n","    </tr>\n","    <tr>\n","      <td>7380</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>7410</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>7440</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>7470</td>\n","      <td>0.001600</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7530</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7560</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7590</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7620</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>7650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7680</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7710</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>7740</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7830</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>7860</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>7890</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7920</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7980</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8010</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8040</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8070</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8130</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8160</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8190</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8220</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8280</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8310</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8340</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8370</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8430</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8460</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8490</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8520</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8580</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8610</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8640</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8670</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8730</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8760</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8790</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8820</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8910</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8940</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8970</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9030</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9060</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9090</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9120</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9180</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9210</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9240</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9270</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9330</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9360</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9390</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9420</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9480</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9510</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9540</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9570</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9630</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9660</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9690</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9720</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9780</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9810</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9840</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9870</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9930</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9960</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10020</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10080</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10110</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10140</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10170</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10230</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>10260</td>\n","      <td>0.002800</td>\n","    </tr>\n","    <tr>\n","      <td>10290</td>\n","      <td>0.001600</td>\n","    </tr>\n","    <tr>\n","      <td>10320</td>\n","      <td>0.002900</td>\n","    </tr>\n","    <tr>\n","      <td>10350</td>\n","      <td>0.001600</td>\n","    </tr>\n","    <tr>\n","      <td>10380</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>10410</td>\n","      <td>0.000400</td>\n","    </tr>\n","    <tr>\n","      <td>10440</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>10470</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>10530</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>10560</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10590</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10620</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10650</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>10680</td>\n","      <td>0.002300</td>\n","    </tr>\n","    <tr>\n","      <td>10710</td>\n","      <td>0.005000</td>\n","    </tr>\n","    <tr>\n","      <td>10740</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>10770</td>\n","      <td>0.002100</td>\n","    </tr>\n","    <tr>\n","      <td>10800</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>10830</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>10860</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10890</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10920</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10980</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11010</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11040</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11070</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>11130</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>11160</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11190</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11220</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11280</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11310</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11340</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11370</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11400</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>11430</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>11460</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11490</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11520</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11580</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11610</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11640</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11670</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11730</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11760</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11790</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11820</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11880</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11910</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11940</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11970</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12030</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12060</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12090</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12120</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12150</td>\n","      <td>0.000900</td>\n","    </tr>\n","    <tr>\n","      <td>12180</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>12210</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12240</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12270</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12330</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12360</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12390</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12420</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12480</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12510</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>12540</td>\n","      <td>0.003200</td>\n","    </tr>\n","    <tr>\n","      <td>12570</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>12600</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>12630</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12660</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12690</td>\n","      <td>0.000600</td>\n","    </tr>\n","    <tr>\n","      <td>12720</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12780</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12810</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12840</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12870</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12900</td>\n","      <td>0.000200</td>\n","    </tr>\n","    <tr>\n","      <td>12930</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>12960</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>12990</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>13020</td>\n","      <td>0.001200</td>\n","    </tr>\n","    <tr>\n","      <td>13050</td>\n","      <td>0.001400</td>\n","    </tr>\n","    <tr>\n","      <td>13080</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13110</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13140</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13170</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13230</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13260</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13290</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13320</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13380</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13410</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13440</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13470</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13530</td>\n","      <td>0.000700</td>\n","    </tr>\n","    <tr>\n","      <td>13560</td>\n","      <td>0.000300</td>\n","    </tr>\n","    <tr>\n","      <td>13590</td>\n","      <td>0.002900</td>\n","    </tr>\n","    <tr>\n","      <td>13620</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>13650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13680</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13710</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13740</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13770</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13830</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13860</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13890</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13920</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13980</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14010</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14040</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14070</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14130</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14160</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14190</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14220</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14250</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>14280</td>\n","      <td>0.000900</td>\n","    </tr>\n","    <tr>\n","      <td>14310</td>\n","      <td>0.000400</td>\n","    </tr>\n","    <tr>\n","      <td>14340</td>\n","      <td>0.001000</td>\n","    </tr>\n","    <tr>\n","      <td>14370</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>14400</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>14430</td>\n","      <td>0.001500</td>\n","    </tr>\n","    <tr>\n","      <td>14460</td>\n","      <td>0.001700</td>\n","    </tr>\n","    <tr>\n","      <td>14490</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>14520</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14580</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14610</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14640</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14670</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14730</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-1000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-1000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-1000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-1500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-1500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-1500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-2000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-2000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-2000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-2500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-2500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-2500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-3000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-3000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-3000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-3500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-3500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-3500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-4000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-4000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-4000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-4500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-4500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-4500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-5000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-5000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-5000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-5500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-5500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-5500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-6000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-6000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-6000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-6500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-6500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-6500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-7000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-7000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-7000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-7500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-7500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-7500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-8000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-8000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-8000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-8500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-8500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-8500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-9000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-9000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-9000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-9500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-9500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-9500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-10000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-10000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-10000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-10500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-10500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-10500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-11000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-11000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-11000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-11500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-11500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-11500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-12000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-12000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-12000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-12500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-12500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-12500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-13000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-13000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-13000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-13500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-13500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-13500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-14000\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-14000/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-14000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","Saving model checkpoint to /content/gdrive/MyDrive/model/checkpoint-14500\n","Configuration saved in /content/gdrive/MyDrive/model/checkpoint-14500/config.json\n","Model weights saved in /content/gdrive/MyDrive/model/checkpoint-14500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:532: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1452\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m                 ):\n\u001b[1;32m   1456\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["trainer.evaluate()\n","# saving the fine tuned model & tokenizer\n","model_path = \"/content/gdrive/MyDrive/models\"\n","model.save_pretrained(model_path)\n","tokenizer.save_pretrained(model_path)"],"metadata":{"id":"ehtzbjUY-avp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651161078762,"user_tz":-540,"elapsed":7945,"user":{"displayName":"조나영","userId":"08548129972931830779"}},"outputId":"22375dea-c58a-4de4-f1b9-6075c8527b70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Configuration saved in /content/gdrive/MyDrive/models/config.json\n","Model weights saved in /content/gdrive/MyDrive/models/pytorch_model.bin\n","tokenizer config file saved in /content/gdrive/MyDrive/models/tokenizer_config.json\n","Special tokens file saved in /content/gdrive/MyDrive/models/special_tokens_map.json\n"]},{"output_type":"execute_result","data":{"text/plain":["('/content/gdrive/MyDrive/models/tokenizer_config.json',\n"," '/content/gdrive/MyDrive/models/special_tokens_map.json',\n"," '/content/gdrive/MyDrive/models/vocab.txt',\n"," '/content/gdrive/MyDrive/models/added_tokens.json')"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["torch.save(model, '/content/gdrive/MyDrive/batch_32_64_model_epoch_3.pt')"],"metadata":{"id":"RA7hafWu-9pR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9k7Ka4H-3Ocf"},"source":["성능을 테스트 해봅시다."]},{"cell_type":"code","metadata":{"id":"mmt7meWE3O1k","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"6b823436-8a8e-49ce-ed65-1b95606aaed1","executionInfo":{"status":"ok","timestamp":1651160857536,"user_tz":-540,"elapsed":1073,"user":{"displayName":"조나영","userId":"08548129972931830779"}}},"source":["sentence = \"\"\"\n","그런데 서로 다른 키 값에 대해 같은 방 번호가 나온다면 어떻게 될까? 키를 통해 얻으려고 하는 값을 얻지 못하거나, \n","엉뚱한 키가 삭제되거나 하는 문제가 발생하게 된다. 이런 경우를 \"해시 충돌\"이라 하며, 해시 충돌을 최대한으로 줄이는 해시 함수의 설계가 해시 테이블의 핵심이라고 볼 수 있다. \n","충돌이 일어나도 해결할 수 있는 방법은 두 가지가 있다. 가장 많이 쓰이는 방법은 체이닝이다. 해시 함수를 통해 얻어지는 방 번호에 해당하는 연결 리스트 등의 자료구조 만들어 놓고, \n","충돌이 일어나면 키 값을 통해 연결리스트를 순회하는 방법이다. 두 번째 방법은 개방 번지화이다. \n","이 방법은 비어있는 공간을 찾아서 할당하는 방법이다. 이번 포스팅에서는 이 방법으로 해시맵을 구현했다.\n","\"\"\"\n","\n","if torch.cuda.is_available():\n","  d = \"cuda\"\n","else:\n","  d = \"cpu\"\n","\n","input_ids = torch.tensor(tokenizer.encode(sentence, add_special_tokens=True)).unsqueeze(0).to(torch.device(d))\n","generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.pad_token_id)\n","tokenizer.decode(generated[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["'[PAD] [CLS] [CLS] [CLS] kb kb kb mb mb mb kb kb16161615151516161717171616141414161613141415151414131413131415141516141317141417141315141316161214141214'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"jGJU5ou6_UE2"},"source":["# inference\n","\n","from transformers import BertForSequenceClassification, BertTokenizerFast\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import fetch_20newsgroups\n","\n","model_path = \"20newsgroups-bert-base-uncased\"\n","max_length = 512\n","\n","def read_20newsgroups(test_size=0.2):\n","  dataset = fetch_20newsgroups(subset=\"all\", shuffle=True, remove=(\"headers\", \"footers\", \"quotes\"))\n","  documents = dataset.data\n","  labels = dataset.target\n","  return train_test_split(documents, labels, test_size=test_size), dataset.target_names\n","\n","\n","(train_texts, valid_texts, train_labels, valid_labels), target_names = read_20newsgroups()\n","\n","model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(target_names)).to(\"cuda\")\n","tokenizer = BertTokenizerFast.from_pretrained(model_path)\n","\n","def get_prediction(text):\n","    # prepare our text into tokenized sequence\n","    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n","    # perform inference to our model\n","    outputs = model(**inputs)\n","    # get output probabilities by doing softmax\n","    probs = outputs[0].softmax(1)\n","    # executing argmax function to get the candidate label\n","    return target_names[probs.argmax()]\n","\n","# Example #1\n","text = \"\"\"With the pace of smartphone evolution moving so fast, there's always something waiting in the wings. \n","No sooner have you spied the latest handset, that there's anticipation for the next big thing. \n","Here we look at those phones that haven't yet launched, the upcoming phones for 2021. \n","We'll be updating this list on a regular basis, with those device rumours we think are credible and exciting.\"\"\"\n","print(get_prediction(text))\n","# Example #2\n","text = \"\"\"\n","A black hole is a place in space where gravity pulls so much that even light can not get out. \n","The gravity is so strong because matter has been squeezed into a tiny space. This can happen when a star is dying.\n","Because no light can get out, people can't see black holes. \n","They are invisible. Space telescopes with special tools can help find black holes. \n","The special tools can see how stars that are very close to black holes act differently than other stars.\n","\"\"\"\n","print(get_prediction(text))"],"execution_count":null,"outputs":[]}]}